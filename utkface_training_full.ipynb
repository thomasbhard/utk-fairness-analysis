{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utkface_training_full.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMlmEhY64wwj"
      },
      "source": [
        "!unzip UTKFaceFull.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ReM53Ld46Gr",
        "outputId": "3b990d43-7714-49a7-a851-f1377081e859"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from PIL import Image\n",
        "\n",
        "dataset_folder_name = 'UTKFace'\n",
        "\n",
        "TRAIN_TEST_SPLIT = 0.7\n",
        "IM_WIDTH = IM_HEIGHT = 198\n",
        "\n",
        "dataset_dict = {\n",
        "    'race_id': {\n",
        "        0: 'white', \n",
        "        1: 'black', \n",
        "        2: 'asian', \n",
        "        3: 'indian', \n",
        "        4: 'others'\n",
        "    },\n",
        "    'gender_id': {\n",
        "        0: 'male',\n",
        "        1: 'female'\n",
        "    }\n",
        "}\n",
        "\n",
        "dataset_dict['gender_alias'] = dict((g, i) for i, g in dataset_dict['gender_id'].items())\n",
        "dataset_dict['race_alias'] = dict((g, i) for i, g in dataset_dict['race_id'].items())\n",
        "\n",
        "\n",
        "def parse_dataset(dataset_path, ext='jpg'):\n",
        "    \"\"\"\n",
        "    Used to extract information about our dataset. It does iterate over all images and return a DataFrame with\n",
        "    the data (age, gender and sex) of all files.\n",
        "    \"\"\"\n",
        "    def parse_info_from_file(path):\n",
        "        \"\"\"\n",
        "        Parse information from a single file\n",
        "        \"\"\"\n",
        "        try:\n",
        "            filename = os.path.split(path)[1]\n",
        "            filename = os.path.splitext(filename)[0]\n",
        "            age, gender, race, _ = filename.split('_')\n",
        "\n",
        "            return int(age), dataset_dict['gender_id'][int(gender)], dataset_dict['race_id'][int(race)]\n",
        "        except Exception as ex:\n",
        "            return None, None, None\n",
        "        \n",
        "    files = glob.glob(os.path.join(dataset_path, \"*.%s\" % ext))\n",
        "    \n",
        "    records = []\n",
        "    for file in files:\n",
        "        info = parse_info_from_file(file)\n",
        "        records.append(info)\n",
        "        \n",
        "    df = pd.DataFrame(records)\n",
        "    df['file'] = files\n",
        "    df.columns = ['age', 'gender', 'race', 'file']\n",
        "    df = df.dropna()\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "df = parse_dataset(dataset_folder_name)\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "class UtkFaceDataGenerator():\n",
        "    \"\"\"\n",
        "    Data generator for the UTKFace dataset. This class should be used when training our Keras multi-output model.\n",
        "    \"\"\"\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        \n",
        "    def generate_split_indexes(self):\n",
        "        p = np.random.permutation(len(self.df))\n",
        "        train_up_to = int(len(self.df) * TRAIN_TEST_SPLIT)\n",
        "        train_idx = p[:train_up_to]\n",
        "        test_idx = p[train_up_to:]\n",
        "\n",
        "        train_up_to = int(train_up_to * TRAIN_TEST_SPLIT)\n",
        "        train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:]\n",
        "        \n",
        "        # converts alias to id\n",
        "        self.df['gender_id'] = self.df['gender'].map(lambda gender: dataset_dict['gender_alias'][gender])\n",
        "        self.df['race_id'] = self.df['race'].map(lambda race: dataset_dict['race_alias'][race])\n",
        "\n",
        "        self.max_age = self.df['age'].max()\n",
        "        \n",
        "        return train_idx, valid_idx, test_idx\n",
        "    \n",
        "    def preprocess_image(self, img_path):\n",
        "        \"\"\"\n",
        "        Used to perform some minor preprocessing on the image before inputting into the network.\n",
        "        \"\"\"\n",
        "        im = Image.open(img_path)\n",
        "        im = im.resize((IM_WIDTH, IM_HEIGHT))\n",
        "        im = np.array(im) / 255.0\n",
        "        \n",
        "        return im\n",
        "        \n",
        "    def generate_images(self, image_idx, is_training, batch_size=16):\n",
        "        \"\"\"\n",
        "        Used to generate a batch with images when training/testing/validating our Keras model.\n",
        "        \"\"\"\n",
        "        \n",
        "        # arrays to store our batched data\n",
        "        images, ages, races, genders = [], [], [], []\n",
        "        while True:\n",
        "            for idx in image_idx:\n",
        "                person = self.df.iloc[idx]\n",
        "                \n",
        "                age = person['age']\n",
        "                race = person['race_id']\n",
        "                gender = person['gender_id']\n",
        "                file = person['file']\n",
        "                \n",
        "                im = self.preprocess_image(file)\n",
        "                \n",
        "                ages.append(age / self.max_age)\n",
        "                races.append(to_categorical(race, len(dataset_dict['race_id'])))\n",
        "                genders.append(to_categorical(gender, len(dataset_dict['gender_id'])))\n",
        "                images.append(im)\n",
        "                \n",
        "                # yielding condition\n",
        "                if len(images) >= batch_size:\n",
        "                    yield np.array(images), [np.array(ages), np.array(races), np.array(genders)]\n",
        "                    images, ages, races, genders = [], [], [], []\n",
        "                    \n",
        "            if not is_training:\n",
        "                break\n",
        "                \n",
        "data_generator = UtkFaceDataGenerator(df)\n",
        "train_idx, valid_idx, test_idx = data_generator.generate_split_indexes()\n",
        "\n",
        "\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Lambda\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Input\n",
        "import tensorflow as tf\n",
        "\n",
        "class UtkMultiOutputModel():\n",
        "    \"\"\"\n",
        "    Used to generate our multi-output model. This CNN contains three branches, one for age, other for \n",
        "    sex and another for race. Each branch contains a sequence of Convolutional Layers that is defined\n",
        "    on the make_default_hidden_layers method.\n",
        "    \"\"\"\n",
        "    def make_default_hidden_layers(self, inputs):\n",
        "        \"\"\"\n",
        "        Used to generate a default set of hidden layers. The structure used in this network is defined as:\n",
        "        \n",
        "        Conv2D -> BatchNormalization -> Pooling -> Dropout\n",
        "        \"\"\"\n",
        "        x = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization(axis=-1)(x)\n",
        "        x = MaxPooling2D(pool_size=(3, 3))(x)\n",
        "        x = Dropout(0.25)(x)\n",
        "\n",
        "        x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization(axis=-1)(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "        x = Dropout(0.25)(x)\n",
        "\n",
        "        x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization(axis=-1)(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "        x = Dropout(0.25)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def build_race_branch(self, inputs, num_races):\n",
        "        \"\"\"\n",
        "        Used to build the race branch of our face recognition network.\n",
        "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
        "        followed by the Dense output layer.\n",
        "        \"\"\"\n",
        "        x = self.make_default_hidden_layers(inputs)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(128)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(num_races)(x)\n",
        "        x = Activation(\"softmax\", name=\"race_output\")(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def build_gender_branch(self, inputs, num_genders=2):\n",
        "        \"\"\"\n",
        "        Used to build the gender branch of our face recognition network.\n",
        "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
        "        followed by the Dense output layer.\n",
        "        \"\"\"\n",
        "        x = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs)\n",
        "\n",
        "        x = self.make_default_hidden_layers(inputs)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(128)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(num_genders)(x)\n",
        "        x = Activation(\"sigmoid\", name=\"gender_output\")(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def build_age_branch(self, inputs):   \n",
        "        \"\"\"\n",
        "        Used to build the age branch of our face recognition network.\n",
        "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
        "        followed by the Dense output layer.\n",
        "\n",
        "        \"\"\"\n",
        "        x = self.make_default_hidden_layers(inputs)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(128)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(1)(x)\n",
        "        x = Activation(\"linear\", name=\"age_output\")(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def assemble_full_model(self, width, height, num_races):\n",
        "        \"\"\"\n",
        "        Used to assemble our multi-output model CNN.\n",
        "        \"\"\"\n",
        "        input_shape = (height, width, 3)\n",
        "\n",
        "        inputs = Input(shape=input_shape)\n",
        "\n",
        "        age_branch = self.build_age_branch(inputs)\n",
        "        race_branch = self.build_race_branch(inputs, num_races)\n",
        "        gender_branch = self.build_gender_branch(inputs)\n",
        "\n",
        "        model = Model(inputs=inputs,\n",
        "                     outputs = [age_branch, race_branch, gender_branch],\n",
        "                     name=\"face_net\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def assemble_gender_only(self, width, height):\n",
        "        input_shape = (height, width, 3)\n",
        "\n",
        "        inputs = Input(shape=input_shape)\n",
        "\n",
        "        gender_branch = self.build_gender_branch(inputs)\n",
        "\n",
        "        model = Model(inputs=inputs,\n",
        "                     outputs = gender_branch,\n",
        "                     name=\"face_net\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    \n",
        "model = UtkMultiOutputModel().assemble_full_model(IM_WIDTH, IM_HEIGHT, num_races=len(dataset_dict['race_alias']))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "init_lr = 1e-4\n",
        "epochs = 50\n",
        "\n",
        "opt = Adam(lr=init_lr, decay=init_lr / epochs)\n",
        "\n",
        "model.compile(optimizer=opt, \n",
        "              loss={\n",
        "                  'age_output': 'mse', \n",
        "                  'race_output': 'categorical_crossentropy', \n",
        "                  'gender_output': 'binary_crossentropy'},\n",
        "              loss_weights={\n",
        "                  'age_output': 4., \n",
        "                  'race_output': 1.5, \n",
        "                  'gender_output': 0.1},\n",
        "              metrics={\n",
        "                  'age_output': 'mae', \n",
        "                  'race_output': 'accuracy',\n",
        "                  'gender_output': 'accuracy'})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    age  gender   race                                           file\n",
            "0  16.0  female  white  UTKFace/16_1_0_20170109214757287.jpg.chip.jpg\n",
            "1  23.0    male  asian  UTKFace/23_0_2_20170116172945780.jpg.chip.jpg\n",
            "2  40.0    male  white  UTKFace/40_0_0_20170117182850349.jpg.chip.jpg\n",
            "3   1.0  female  white   UTKFace/1_1_0_20170109191449860.jpg.chip.jpg\n",
            "4  28.0    male  asian  UTKFace/28_0_2_20170104021218340.jpg.chip.jpg\n",
            "Model: \"face_net\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 198, 198, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 198, 198, 16) 448         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 198, 198, 16) 448         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 198, 198, 16) 448         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 198, 198, 16) 0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 198, 198, 16) 0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 198, 198, 16) 0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 198, 198, 16) 64          activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 198, 198, 16) 64          activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 198, 198, 16) 64          activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 66, 66, 16)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 66, 66, 16)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling2D) (None, 66, 66, 16)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 66, 66, 16)   0           max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 66, 66, 16)   0           max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 66, 66, 16)   0           max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 66, 66, 32)   4640        dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 66, 66, 32)   4640        dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 66, 66, 32)   4640        dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 66, 66, 32)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 66, 66, 32)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 66, 66, 32)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 66, 66, 32)   128         activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 66, 66, 32)   128         activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 66, 66, 32)   128         activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 33, 33, 32)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling2D) (None, 33, 33, 32)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling2D) (None, 33, 33, 32)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 33, 33, 32)   0           max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 33, 33, 32)   0           max_pooling2d_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 33, 33, 32)   0           max_pooling2d_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 33, 33, 32)   9248        dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 33, 33, 32)   9248        dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 33, 33, 32)   9248        dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 33, 33, 32)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 33, 33, 32)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 33, 33, 32)   0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 33, 33, 32)   128         activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 33, 33, 32)   128         activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 33, 33, 32)   128         activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 16, 16, 32)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling2D) (None, 16, 16, 32)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling2D) (None, 16, 16, 32)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 16, 16, 32)   0           max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 16, 16, 32)   0           max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 16, 16, 32)   0           max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 8192)         0           dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 8192)         0           dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 8192)         0           dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          1048704     flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 128)          1048704     flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 128)          1048704     flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 128)          0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 128)          0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 128)          0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 128)          512         activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 128)          512         activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 128)          512         activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 128)          0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 128)          0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 128)          0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            129         dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 5)            645         dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 2)            258         dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Activation)         (None, 1)            0           dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "race_output (Activation)        (None, 5)            0           dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Activation)      (None, 2)            0           dense_11[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 3,192,648\n",
            "Trainable params: 3,191,400\n",
            "Non-trainable params: 1,248\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXmta98h5AeB",
        "outputId": "fbd81f55-b30a-44ba-fcf2-58fb5593ddcd"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "batch_size = 32\n",
        "valid_batch_size = 32\n",
        "train_gen = data_generator.generate_images(train_idx, is_training=True, batch_size=batch_size)\n",
        "valid_gen = data_generator.generate_images(valid_idx, is_training=True, batch_size=valid_batch_size)\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\"./model_checkpoint\", monitor='val_loss')\n",
        "]\n",
        "\n",
        "history = model.fit(train_gen,\n",
        "                    steps_per_epoch=len(train_idx)//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_data=valid_gen,\n",
        "                    validation_steps=len(valid_idx)//valid_batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "362/362 [==============================] - 99s 147ms/step - loss: 15.2998 - age_output_loss: 3.0447 - race_output_loss: 2.0295 - gender_output_loss: 0.7653 - age_output_mae: 1.3621 - race_output_accuracy: 0.3561 - gender_output_accuracy: 0.6761 - val_loss: 38.3992 - val_age_output_loss: 7.9177 - val_race_output_loss: 4.3018 - val_gender_output_loss: 2.7599 - val_age_output_mae: 2.5737 - val_race_output_accuracy: 0.4292 - val_gender_output_accuracy: 0.4843\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 2/50\n",
            "362/362 [==============================] - 52s 142ms/step - loss: 8.6252 - age_output_loss: 1.6395 - race_output_loss: 1.3442 - gender_output_loss: 0.5082 - age_output_mae: 0.9965 - race_output_accuracy: 0.5464 - gender_output_accuracy: 0.8005 - val_loss: 87.4606 - val_age_output_loss: 21.1034 - val_race_output_loss: 1.9699 - val_gender_output_loss: 0.9205 - val_age_output_mae: 3.1136 - val_race_output_accuracy: 0.5224 - val_gender_output_accuracy: 0.7718\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 3/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 6.5242 - age_output_loss: 1.1785 - race_output_loss: 1.1767 - gender_output_loss: 0.4524 - age_output_mae: 0.8244 - race_output_accuracy: 0.6049 - gender_output_accuracy: 0.8327 - val_loss: 137.5722 - val_age_output_loss: 33.7338 - val_race_output_loss: 1.7157 - val_gender_output_loss: 0.6326 - val_age_output_mae: 3.8429 - val_race_output_accuracy: 0.5581 - val_gender_output_accuracy: 0.8040\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 4/50\n",
            "362/362 [==============================] - 51s 142ms/step - loss: 5.3552 - age_output_loss: 0.9352 - race_output_loss: 1.0487 - gender_output_loss: 0.4145 - age_output_mae: 0.7240 - race_output_accuracy: 0.6479 - gender_output_accuracy: 0.8400 - val_loss: 283.3686 - val_age_output_loss: 70.1544 - val_race_output_loss: 1.8002 - val_gender_output_loss: 0.5065 - val_age_output_mae: 5.8375 - val_race_output_accuracy: 0.5575 - val_gender_output_accuracy: 0.8353\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 5/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 4.4740 - age_output_loss: 0.7418 - race_output_loss: 0.9791 - gender_output_loss: 0.3795 - age_output_mae: 0.6493 - race_output_accuracy: 0.6729 - gender_output_accuracy: 0.8554 - val_loss: 117.0469 - val_age_output_loss: 28.7449 - val_race_output_loss: 1.3428 - val_gender_output_loss: 0.5302 - val_age_output_mae: 3.3319 - val_race_output_accuracy: 0.5996 - val_gender_output_accuracy: 0.8379\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 6/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 3.7613 - age_output_loss: 0.5887 - race_output_loss: 0.9143 - gender_output_loss: 0.3516 - age_output_mae: 0.5791 - race_output_accuracy: 0.6892 - gender_output_accuracy: 0.8657 - val_loss: 104.7867 - val_age_output_loss: 25.6735 - val_race_output_loss: 1.3672 - val_gender_output_loss: 0.4175 - val_age_output_mae: 3.1474 - val_race_output_accuracy: 0.6034 - val_gender_output_accuracy: 0.8581\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 7/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 3.1175 - age_output_loss: 0.4477 - race_output_loss: 0.8619 - gender_output_loss: 0.3373 - age_output_mae: 0.5053 - race_output_accuracy: 0.7073 - gender_output_accuracy: 0.8678 - val_loss: 117.9319 - val_age_output_loss: 29.0825 - val_race_output_loss: 1.0394 - val_gender_output_loss: 0.4311 - val_age_output_mae: 3.3696 - val_race_output_accuracy: 0.6657 - val_gender_output_accuracy: 0.8565\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 8/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 2.8207 - age_output_loss: 0.3881 - race_output_loss: 0.8235 - gender_output_loss: 0.3293 - age_output_mae: 0.4585 - race_output_accuracy: 0.7145 - gender_output_accuracy: 0.8696 - val_loss: 162.1317 - val_age_output_loss: 40.0441 - val_race_output_loss: 1.2756 - val_gender_output_loss: 0.4194 - val_age_output_mae: 3.7340 - val_race_output_accuracy: 0.6232 - val_gender_output_accuracy: 0.8639\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 9/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 2.3954 - age_output_loss: 0.2976 - race_output_loss: 0.7826 - gender_output_loss: 0.3110 - age_output_mae: 0.4084 - race_output_accuracy: 0.7286 - gender_output_accuracy: 0.8729 - val_loss: 243.3614 - val_age_output_loss: 60.4185 - val_race_output_loss: 1.0997 - val_gender_output_loss: 0.3792 - val_age_output_mae: 4.8157 - val_race_output_accuracy: 0.6716 - val_gender_output_accuracy: 0.8740\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 10/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 2.0683 - age_output_loss: 0.2275 - race_output_loss: 0.7521 - gender_output_loss: 0.2992 - age_output_mae: 0.3548 - race_output_accuracy: 0.7381 - gender_output_accuracy: 0.8767 - val_loss: 166.8497 - val_age_output_loss: 41.2819 - val_race_output_loss: 1.1268 - val_gender_output_loss: 0.3174 - val_age_output_mae: 4.1740 - val_race_output_accuracy: 0.6788 - val_gender_output_accuracy: 0.8802\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 11/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 1.8112 - age_output_loss: 0.1716 - race_output_loss: 0.7303 - gender_output_loss: 0.2913 - age_output_mae: 0.3091 - race_output_accuracy: 0.7456 - gender_output_accuracy: 0.8823 - val_loss: 149.5598 - val_age_output_loss: 36.9677 - val_race_output_loss: 1.1040 - val_gender_output_loss: 0.3302 - val_age_output_mae: 3.8394 - val_race_output_accuracy: 0.6772 - val_gender_output_accuracy: 0.8788\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 12/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 1.6182 - age_output_loss: 0.1351 - race_output_loss: 0.6998 - gender_output_loss: 0.2816 - age_output_mae: 0.2737 - race_output_accuracy: 0.7594 - gender_output_accuracy: 0.8843 - val_loss: 175.2152 - val_age_output_loss: 43.3187 - val_race_output_loss: 1.2712 - val_gender_output_loss: 0.3373 - val_age_output_mae: 4.0367 - val_race_output_accuracy: 0.6484 - val_gender_output_accuracy: 0.8796\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 13/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 1.4341 - age_output_loss: 0.1020 - race_output_loss: 0.6664 - gender_output_loss: 0.2659 - age_output_mae: 0.2397 - race_output_accuracy: 0.7649 - gender_output_accuracy: 0.8890 - val_loss: 149.1637 - val_age_output_loss: 36.8938 - val_race_output_loss: 1.0375 - val_gender_output_loss: 0.3221 - val_age_output_mae: 3.7008 - val_race_output_accuracy: 0.6925 - val_gender_output_accuracy: 0.8728\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 14/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 1.3044 - age_output_loss: 0.0777 - race_output_loss: 0.6447 - gender_output_loss: 0.2654 - age_output_mae: 0.2083 - race_output_accuracy: 0.7745 - gender_output_accuracy: 0.8902 - val_loss: 160.8329 - val_age_output_loss: 39.8320 - val_race_output_loss: 0.9770 - val_gender_output_loss: 0.3946 - val_age_output_mae: 3.9299 - val_race_output_accuracy: 0.6994 - val_gender_output_accuracy: 0.8631\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 15/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 1.2421 - age_output_loss: 0.0587 - race_output_loss: 0.6545 - gender_output_loss: 0.2569 - age_output_mae: 0.1812 - race_output_accuracy: 0.7738 - gender_output_accuracy: 0.8969 - val_loss: 124.4218 - val_age_output_loss: 30.6730 - val_race_output_loss: 1.1335 - val_gender_output_loss: 0.2945 - val_age_output_mae: 3.3836 - val_race_output_accuracy: 0.6780 - val_gender_output_accuracy: 0.8806\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 16/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 1.1537 - age_output_loss: 0.0432 - race_output_loss: 0.6369 - gender_output_loss: 0.2539 - age_output_mae: 0.1603 - race_output_accuracy: 0.7811 - gender_output_accuracy: 0.8945 - val_loss: 107.9863 - val_age_output_loss: 26.6595 - val_race_output_loss: 0.8795 - val_gender_output_loss: 0.2928 - val_age_output_mae: 3.1183 - val_race_output_accuracy: 0.7258 - val_gender_output_accuracy: 0.8871\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 17/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 1.0860 - age_output_loss: 0.0349 - race_output_loss: 0.6148 - gender_output_loss: 0.2416 - age_output_mae: 0.1436 - race_output_accuracy: 0.7811 - gender_output_accuracy: 0.9012 - val_loss: 82.9179 - val_age_output_loss: 20.3943 - val_race_output_loss: 0.8748 - val_gender_output_loss: 0.2852 - val_age_output_mae: 2.6259 - val_race_output_accuracy: 0.7228 - val_gender_output_accuracy: 0.8889\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 18/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 1.0434 - age_output_loss: 0.0286 - race_output_loss: 0.6032 - gender_output_loss: 0.2434 - age_output_mae: 0.1301 - race_output_accuracy: 0.7897 - gender_output_accuracy: 0.8973 - val_loss: 61.0772 - val_age_output_loss: 14.9664 - val_race_output_loss: 0.7890 - val_gender_output_loss: 0.2809 - val_age_output_mae: 2.1397 - val_race_output_accuracy: 0.7448 - val_gender_output_accuracy: 0.8931\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 19/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.9903 - age_output_loss: 0.0238 - race_output_loss: 0.5816 - gender_output_loss: 0.2280 - age_output_mae: 0.1191 - race_output_accuracy: 0.7973 - gender_output_accuracy: 0.9113 - val_loss: 64.7772 - val_age_output_loss: 15.8863 - val_race_output_loss: 0.8027 - val_gender_output_loss: 0.2766 - val_age_output_mae: 2.3291 - val_race_output_accuracy: 0.7435 - val_gender_output_accuracy: 0.8944\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 20/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 0.9684 - age_output_loss: 0.0219 - race_output_loss: 0.5724 - gender_output_loss: 0.2228 - age_output_mae: 0.1132 - race_output_accuracy: 0.7953 - gender_output_accuracy: 0.9102 - val_loss: 107.7624 - val_age_output_loss: 26.6155 - val_race_output_loss: 0.8480 - val_gender_output_loss: 0.2829 - val_age_output_mae: 3.1030 - val_race_output_accuracy: 0.7427 - val_gender_output_accuracy: 0.8933\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 21/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.9552 - age_output_loss: 0.0194 - race_output_loss: 0.5711 - gender_output_loss: 0.2102 - age_output_mae: 0.1068 - race_output_accuracy: 0.7971 - gender_output_accuracy: 0.9146 - val_loss: 115.5534 - val_age_output_loss: 28.5920 - val_race_output_loss: 0.7719 - val_gender_output_loss: 0.2769 - val_age_output_mae: 3.3452 - val_race_output_accuracy: 0.7522 - val_gender_output_accuracy: 0.8901\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 22/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 0.8998 - age_output_loss: 0.0179 - race_output_loss: 0.5384 - gender_output_loss: 0.2060 - age_output_mae: 0.1026 - race_output_accuracy: 0.8042 - gender_output_accuracy: 0.9131 - val_loss: 97.6199 - val_age_output_loss: 24.1290 - val_race_output_loss: 0.7171 - val_gender_output_loss: 0.2806 - val_age_output_mae: 2.9394 - val_race_output_accuracy: 0.7601 - val_gender_output_accuracy: 0.8925\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 23/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.8790 - age_output_loss: 0.0171 - race_output_loss: 0.5267 - gender_output_loss: 0.2062 - age_output_mae: 0.0999 - race_output_accuracy: 0.8162 - gender_output_accuracy: 0.9160 - val_loss: 71.9306 - val_age_output_loss: 17.7083 - val_race_output_loss: 0.7117 - val_gender_output_loss: 0.2970 - val_age_output_mae: 2.3605 - val_race_output_accuracy: 0.7694 - val_gender_output_accuracy: 0.8919\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 24/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 0.8597 - age_output_loss: 0.0164 - race_output_loss: 0.5159 - gender_output_loss: 0.2025 - age_output_mae: 0.0976 - race_output_accuracy: 0.8191 - gender_output_accuracy: 0.9157 - val_loss: 58.2504 - val_age_output_loss: 14.2862 - val_race_output_loss: 0.7192 - val_gender_output_loss: 0.2692 - val_age_output_mae: 2.0021 - val_race_output_accuracy: 0.7694 - val_gender_output_accuracy: 0.8917\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 25/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 0.8704 - age_output_loss: 0.0159 - race_output_loss: 0.5249 - gender_output_loss: 0.1927 - age_output_mae: 0.0962 - race_output_accuracy: 0.8116 - gender_output_accuracy: 0.9211 - val_loss: 42.6460 - val_age_output_loss: 10.3993 - val_race_output_loss: 0.6809 - val_gender_output_loss: 0.2747 - val_age_output_mae: 1.5789 - val_race_output_accuracy: 0.7800 - val_gender_output_accuracy: 0.8911\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 26/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 0.8284 - age_output_loss: 0.0158 - race_output_loss: 0.4972 - gender_output_loss: 0.1935 - age_output_mae: 0.0955 - race_output_accuracy: 0.8276 - gender_output_accuracy: 0.9233 - val_loss: 27.6370 - val_age_output_loss: 6.6491 - val_race_output_loss: 0.6755 - val_gender_output_loss: 0.2740 - val_age_output_mae: 1.1035 - val_race_output_accuracy: 0.7788 - val_gender_output_accuracy: 0.8927\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 27/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 0.8114 - age_output_loss: 0.0154 - race_output_loss: 0.4878 - gender_output_loss: 0.1833 - age_output_mae: 0.0940 - race_output_accuracy: 0.8276 - gender_output_accuracy: 0.9271 - val_loss: 15.2816 - val_age_output_loss: 3.5629 - val_race_output_loss: 0.6688 - val_gender_output_loss: 0.2662 - val_age_output_mae: 0.6960 - val_race_output_accuracy: 0.7782 - val_gender_output_accuracy: 0.8984\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 28/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 0.7934 - age_output_loss: 0.0144 - race_output_loss: 0.4786 - gender_output_loss: 0.1794 - age_output_mae: 0.0914 - race_output_accuracy: 0.8330 - gender_output_accuracy: 0.9312 - val_loss: 17.4577 - val_age_output_loss: 4.0771 - val_race_output_loss: 0.7486 - val_gender_output_loss: 0.2653 - val_age_output_mae: 0.7389 - val_race_output_accuracy: 0.7649 - val_gender_output_accuracy: 0.8972\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 29/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 0.7885 - age_output_loss: 0.0137 - race_output_loss: 0.4777 - gender_output_loss: 0.1727 - age_output_mae: 0.0894 - race_output_accuracy: 0.8289 - gender_output_accuracy: 0.9328 - val_loss: 22.2601 - val_age_output_loss: 5.2927 - val_race_output_loss: 0.7084 - val_gender_output_loss: 0.2659 - val_age_output_mae: 0.9160 - val_race_output_accuracy: 0.7730 - val_gender_output_accuracy: 0.8972\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 30/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.7597 - age_output_loss: 0.0134 - race_output_loss: 0.4597 - gender_output_loss: 0.1676 - age_output_mae: 0.0885 - race_output_accuracy: 0.8390 - gender_output_accuracy: 0.9362 - val_loss: 10.3889 - val_age_output_loss: 2.3460 - val_race_output_loss: 0.6518 - val_gender_output_loss: 0.2729 - val_age_output_mae: 0.5124 - val_race_output_accuracy: 0.7833 - val_gender_output_accuracy: 0.8968\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 31/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 0.7337 - age_output_loss: 0.0133 - race_output_loss: 0.4429 - gender_output_loss: 0.1626 - age_output_mae: 0.0880 - race_output_accuracy: 0.8491 - gender_output_accuracy: 0.9356 - val_loss: 16.0893 - val_age_output_loss: 3.7695 - val_race_output_loss: 0.6564 - val_gender_output_loss: 0.2668 - val_age_output_mae: 0.6831 - val_race_output_accuracy: 0.7871 - val_gender_output_accuracy: 0.9002\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 32/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.7309 - age_output_loss: 0.0125 - race_output_loss: 0.4435 - gender_output_loss: 0.1561 - age_output_mae: 0.0851 - race_output_accuracy: 0.8453 - gender_output_accuracy: 0.9399 - val_loss: 10.8017 - val_age_output_loss: 2.4241 - val_race_output_loss: 0.7192 - val_gender_output_loss: 0.2653 - val_age_output_mae: 0.4951 - val_race_output_accuracy: 0.7778 - val_gender_output_accuracy: 0.8984\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 33/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 0.7092 - age_output_loss: 0.0125 - race_output_loss: 0.4295 - gender_output_loss: 0.1500 - age_output_mae: 0.0852 - race_output_accuracy: 0.8457 - gender_output_accuracy: 0.9421 - val_loss: 12.7683 - val_age_output_loss: 2.9242 - val_race_output_loss: 0.6964 - val_gender_output_loss: 0.2680 - val_age_output_mae: 0.5562 - val_race_output_accuracy: 0.7780 - val_gender_output_accuracy: 0.8948\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 34/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.6860 - age_output_loss: 0.0118 - race_output_loss: 0.4161 - gender_output_loss: 0.1464 - age_output_mae: 0.0827 - race_output_accuracy: 0.8527 - gender_output_accuracy: 0.9424 - val_loss: 28.7313 - val_age_output_loss: 6.9174 - val_race_output_loss: 0.6896 - val_gender_output_loss: 0.2735 - val_age_output_mae: 0.7847 - val_race_output_accuracy: 0.7825 - val_gender_output_accuracy: 0.8994\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 35/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.6993 - age_output_loss: 0.0129 - race_output_loss: 0.4223 - gender_output_loss: 0.1415 - age_output_mae: 0.0850 - race_output_accuracy: 0.8498 - gender_output_accuracy: 0.9494 - val_loss: 12.7615 - val_age_output_loss: 2.9049 - val_race_output_loss: 0.7432 - val_gender_output_loss: 0.2691 - val_age_output_mae: 0.5959 - val_race_output_accuracy: 0.7698 - val_gender_output_accuracy: 0.8964\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 36/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.6603 - age_output_loss: 0.0115 - race_output_loss: 0.4000 - gender_output_loss: 0.1417 - age_output_mae: 0.0823 - race_output_accuracy: 0.8595 - gender_output_accuracy: 0.9426 - val_loss: 11.0316 - val_age_output_loss: 2.4921 - val_race_output_loss: 0.6900 - val_gender_output_loss: 0.2826 - val_age_output_mae: 0.5508 - val_race_output_accuracy: 0.7843 - val_gender_output_accuracy: 0.8962\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 37/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.6365 - age_output_loss: 0.0112 - race_output_loss: 0.3852 - gender_output_loss: 0.1409 - age_output_mae: 0.0805 - race_output_accuracy: 0.8650 - gender_output_accuracy: 0.9459 - val_loss: 43.5014 - val_age_output_loss: 10.6154 - val_race_output_loss: 0.6747 - val_gender_output_loss: 0.2770 - val_age_output_mae: 0.7871 - val_race_output_accuracy: 0.7873 - val_gender_output_accuracy: 0.8980\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 38/50\n",
            "362/362 [==============================] - 52s 143ms/step - loss: 0.6183 - age_output_loss: 0.0111 - race_output_loss: 0.3741 - gender_output_loss: 0.1293 - age_output_mae: 0.0807 - race_output_accuracy: 0.8673 - gender_output_accuracy: 0.9580 - val_loss: 28.3667 - val_age_output_loss: 6.8183 - val_race_output_loss: 0.7106 - val_gender_output_loss: 0.2762 - val_age_output_mae: 0.6265 - val_race_output_accuracy: 0.7790 - val_gender_output_accuracy: 0.8978\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 39/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.6005 - age_output_loss: 0.0109 - race_output_loss: 0.3626 - gender_output_loss: 0.1279 - age_output_mae: 0.0800 - race_output_accuracy: 0.8741 - gender_output_accuracy: 0.9470 - val_loss: 17.4836 - val_age_output_loss: 4.1074 - val_race_output_loss: 0.6843 - val_gender_output_loss: 0.2727 - val_age_output_mae: 0.4841 - val_race_output_accuracy: 0.7905 - val_gender_output_accuracy: 0.8994\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 40/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.6160 - age_output_loss: 0.0108 - race_output_loss: 0.3735 - gender_output_loss: 0.1259 - age_output_mae: 0.0792 - race_output_accuracy: 0.8633 - gender_output_accuracy: 0.9532 - val_loss: 14.2462 - val_age_output_loss: 3.2957 - val_race_output_loss: 0.6903 - val_gender_output_loss: 0.2787 - val_age_output_mae: 0.4304 - val_race_output_accuracy: 0.7851 - val_gender_output_accuracy: 0.8962\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 41/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.6022 - age_output_loss: 0.0106 - race_output_loss: 0.3649 - gender_output_loss: 0.1261 - age_output_mae: 0.0792 - race_output_accuracy: 0.8656 - gender_output_accuracy: 0.9529 - val_loss: 15.0857 - val_age_output_loss: 3.5076 - val_race_output_loss: 0.6848 - val_gender_output_loss: 0.2810 - val_age_output_mae: 0.3959 - val_race_output_accuracy: 0.7869 - val_gender_output_accuracy: 0.8974\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 42/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.5783 - age_output_loss: 0.0102 - race_output_loss: 0.3509 - gender_output_loss: 0.1133 - age_output_mae: 0.0771 - race_output_accuracy: 0.8736 - gender_output_accuracy: 0.9590 - val_loss: 23.6373 - val_age_output_loss: 5.6466 - val_race_output_loss: 0.6817 - val_gender_output_loss: 0.2841 - val_age_output_mae: 0.4528 - val_race_output_accuracy: 0.7863 - val_gender_output_accuracy: 0.8990\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 43/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.5778 - age_output_loss: 0.0100 - race_output_loss: 0.3502 - gender_output_loss: 0.1246 - age_output_mae: 0.0769 - race_output_accuracy: 0.8725 - gender_output_accuracy: 0.9531 - val_loss: 23.6928 - val_age_output_loss: 5.6450 - val_race_output_loss: 0.7232 - val_gender_output_loss: 0.2804 - val_age_output_mae: 0.4394 - val_race_output_accuracy: 0.7784 - val_gender_output_accuracy: 0.8988\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 44/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.5518 - age_output_loss: 0.0098 - race_output_loss: 0.3340 - gender_output_loss: 0.1156 - age_output_mae: 0.0755 - race_output_accuracy: 0.8832 - gender_output_accuracy: 0.9565 - val_loss: 11.0589 - val_age_output_loss: 2.4622 - val_race_output_loss: 0.7875 - val_gender_output_loss: 0.2861 - val_age_output_mae: 0.2991 - val_race_output_accuracy: 0.7681 - val_gender_output_accuracy: 0.8990\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 45/50\n",
            "362/362 [==============================] - 52s 145ms/step - loss: 0.5548 - age_output_loss: 0.0098 - race_output_loss: 0.3360 - gender_output_loss: 0.1178 - age_output_mae: 0.0753 - race_output_accuracy: 0.8812 - gender_output_accuracy: 0.9557 - val_loss: 7.9219 - val_age_output_loss: 1.7088 - val_race_output_loss: 0.7053 - val_gender_output_loss: 0.2869 - val_age_output_mae: 0.2426 - val_race_output_accuracy: 0.7883 - val_gender_output_accuracy: 0.9014\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 46/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.5260 - age_output_loss: 0.0098 - race_output_loss: 0.3174 - gender_output_loss: 0.1080 - age_output_mae: 0.0752 - race_output_accuracy: 0.8875 - gender_output_accuracy: 0.9611 - val_loss: 8.8234 - val_age_output_loss: 1.9150 - val_race_output_loss: 0.7559 - val_gender_output_loss: 0.2936 - val_age_output_mae: 0.2637 - val_race_output_accuracy: 0.7796 - val_gender_output_accuracy: 0.8994\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 47/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.4875 - age_output_loss: 0.0094 - race_output_loss: 0.2929 - gender_output_loss: 0.1042 - age_output_mae: 0.0739 - race_output_accuracy: 0.8975 - gender_output_accuracy: 0.9619 - val_loss: 4.0125 - val_age_output_loss: 0.7253 - val_race_output_loss: 0.7205 - val_gender_output_loss: 0.3038 - val_age_output_mae: 0.1677 - val_race_output_accuracy: 0.7865 - val_gender_output_accuracy: 0.8992\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 48/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.5082 - age_output_loss: 0.0095 - race_output_loss: 0.3066 - gender_output_loss: 0.1043 - age_output_mae: 0.0740 - race_output_accuracy: 0.8886 - gender_output_accuracy: 0.9625 - val_loss: 2.8620 - val_age_output_loss: 0.4092 - val_race_output_loss: 0.7975 - val_gender_output_loss: 0.2907 - val_age_output_mae: 0.1226 - val_race_output_accuracy: 0.7667 - val_gender_output_accuracy: 0.8990\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 49/50\n",
            "362/362 [==============================] - 52s 144ms/step - loss: 0.4999 - age_output_loss: 0.0090 - race_output_loss: 0.3018 - gender_output_loss: 0.1095 - age_output_mae: 0.0729 - race_output_accuracy: 0.8956 - gender_output_accuracy: 0.9588 - val_loss: 3.0392 - val_age_output_loss: 0.4852 - val_race_output_loss: 0.7118 - val_gender_output_loss: 0.3059 - val_age_output_mae: 0.1250 - val_race_output_accuracy: 0.7899 - val_gender_output_accuracy: 0.8988\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 50/50\n",
            "362/362 [==============================] - 52s 145ms/step - loss: 0.4921 - age_output_loss: 0.0092 - race_output_loss: 0.2971 - gender_output_loss: 0.0956 - age_output_mae: 0.0732 - race_output_accuracy: 0.8915 - gender_output_accuracy: 0.9663 - val_loss: 3.2201 - val_age_output_loss: 0.5253 - val_race_output_loss: 0.7263 - val_gender_output_loss: 0.2931 - val_age_output_mae: 0.1279 - val_race_output_accuracy: 0.7873 - val_gender_output_accuracy: 0.9000\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2ny3d36AaDg"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Fij3EaUYKK",
        "outputId": "064f3574-56d4-4354-feba-be54148ff998"
      },
      "source": [
        "test_batch_size = 128\n",
        "test_generator = data_generator.generate_images(test_idx, is_training=False, batch_size=test_batch_size)\n",
        "age_pred, race_pred, gender_pred = model.predict_generator(test_generator, \n",
        "                                                           steps=len(test_idx)//test_batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1976: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyEiqloIU88y"
      },
      "source": [
        "test_generator = data_generator.generate_images(test_idx, is_training=False, batch_size=test_batch_size)\n",
        "samples = 0\n",
        "images, age_true, race_true, gender_true = [], [], [], []\n",
        "for test_batch in test_generator:\n",
        "    image = test_batch[0]\n",
        "    labels = test_batch[1]\n",
        "    \n",
        "    images.extend(image)\n",
        "    age_true.extend(labels[0])\n",
        "    race_true.extend(labels[1])\n",
        "    gender_true.extend(labels[2])\n",
        "    \n",
        "age_true = np.array(age_true)\n",
        "race_true = np.array(race_true)\n",
        "gender_true = np.array(gender_true)\n",
        "\n",
        "race_true, gender_true = race_true.argmax(axis=-1), gender_true.argmax(axis=-1)\n",
        "race_pred, gender_pred = race_pred.argmax(axis=-1), gender_pred.argmax(axis=-1)\n",
        "\n",
        "age_true = age_true * data_generator.max_age\n",
        "age_pred = age_pred * data_generator.max_age"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOdJh_kMVrka",
        "outputId": "c0fddfaf-d762-402f-8e3a-f0f70b52a075"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cr_gender = classification_report(gender_true, gender_pred, target_names=dataset_dict['gender_alias'].keys())\n",
        "print(cr_gender)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        male       0.90      0.91      0.90      3608\n",
            "      female       0.90      0.89      0.90      3432\n",
            "\n",
            "    accuracy                           0.90      7040\n",
            "   macro avg       0.90      0.90      0.90      7040\n",
            "weighted avg       0.90      0.90      0.90      7040\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocLsB8OzVm_C"
      },
      "source": [
        "age_pred_flat = age_pred.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "gZ3Aw7PGXzxS",
        "outputId": "ce163efd-ddd7-4793-a5e8-c216ea768304"
      },
      "source": [
        "df_prediction = pd.DataFrame({'age_true': age_true, 'age_pred': age_pred_flat, 'race_true': race_true, 'race_pred': race_pred, 'gender_true': gender_true, 'gender_pred': gender_pred})\n",
        "df_prediction = df_prediction.round(0).astype(int)\n",
        "df_prediction.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age_true</th>\n",
              "      <th>age_pred</th>\n",
              "      <th>race_true</th>\n",
              "      <th>race_pred</th>\n",
              "      <th>gender_true</th>\n",
              "      <th>gender_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>26</td>\n",
              "      <td>35</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24</td>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23</td>\n",
              "      <td>36</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>42</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age_true  age_pred  race_true  race_pred  gender_true  gender_pred\n",
              "0        30        29          0          0            0            1\n",
              "1        26        35          4          1            1            1\n",
              "2        24        27          2          0            1            1\n",
              "3        23        36          3          0            0            0\n",
              "4        42        36          0          0            1            1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niYf_nJcYv-B"
      },
      "source": [
        "df_prediction.to_csv('df_predctions_all.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHa45NG5IH2h"
      },
      "source": [
        "test_im = data_generator.preprocess_image('/content/23_1_0_20170104021538021.jpg.chip.jpg')\n",
        "test_im = np.array([test_im])\n",
        "\n",
        "prediction = model.predict(test_im)\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}