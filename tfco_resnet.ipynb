{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TensorflowConstrainedOptimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1BUpZ52uxdb"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U pip==20.2\n",
        "\n",
        "!pip install git+https://github.com/google-research/tensorflow_constrained_optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHhDshmlFUMP",
        "outputId": "4d62b81c-2547-43aa-ef77-f8b59e29063b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmoWYl67FlDE"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/pathto/UTKFace28.zip -d /content/UTKFace28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GFrxJ1nFTbc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS22S9CH82rn"
      },
      "outputs": [],
      "source": [
        "def utk_generator(include_labels=True, include_groups=True, train=True, info=False):\n",
        "    paths = glob.glob(\"UTKFace28/UTKFace28/*\")\n",
        "\n",
        "    files_train, files_test = train_test_split(paths, train_size=0.7, random_state=43)\n",
        "    if train:\n",
        "        paths = files_train\n",
        "    else:\n",
        "        paths = files_test\n",
        "\n",
        "    for path in paths:\n",
        "        try: \n",
        "            filename = os.path.split(path)[1]\n",
        "            filename = os.path.splitext(filename)[0]\n",
        "            age, gender, race, _ = filename.split('_')\n",
        "\n",
        "            labels = (int(gender), int(age), int(race))\n",
        "\n",
        "            img = tf.io.read_file(path)\n",
        "            img = tf.image.decode_jpeg(img)\n",
        "            img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "            label = labels[0]\n",
        "            \n",
        "            group = 1 if race == 2 else 0\n",
        "\n",
        "            if info:\n",
        "              yield (filename, gender, age, race)\n",
        "\n",
        "            elif include_groups:\n",
        "                yield img, label, group\n",
        "            elif include_labels:\n",
        "                yield img, label\n",
        "            else:\n",
        "                yield img\n",
        "        except ValueError as ex:\n",
        "            continue\n",
        "\n",
        "def generate_utk_dataset(include_labels=True, include_groups=True, train=True):\n",
        "    if include_groups:\n",
        "        return tf.data.Dataset.from_generator(utk_generator, args=[True, True, train], output_types=(tf.float32, tf.float32, tf.float32), output_shapes = ((28,28,3), (), ()),)\n",
        "    elif include_labels:\n",
        "        return tf.data.Dataset.from_generator(utk_generator, args=[True, False, train], output_types=(tf.float32, tf.float32), output_shapes = ((28,28,3), ()),)\n",
        "    else:\n",
        "        return tf.data.Dataset.from_generator(utk_generator, args=[False, False, train], output_types=(tf.float32), output_shapes = (28,28,3),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mjVV98iFTbg"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "img_size = (28, 28)\n",
        "\n",
        "\n",
        "train_ds = generate_utk_dataset(include_labels=True, include_groups=False, train=True)\n",
        "test_ds = generate_utk_dataset(include_labels=True, include_groups=False, train=False)\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1024).repeat().batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().batch(1).prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzpSQOqssMon"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "  model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v1_50/feature_vector/5\",\n",
        "                    trainable=True),  # Can be True, see below.\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  model.build([None, 28, 28, 3])  # Batch input shape.\n",
        "\n",
        "  base_learning_rate = 0.0001\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss='mse',\n",
        "              metrics=['mae', 'acc'])\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqkC0TS1FTbj",
        "outputId": "0cacab9a-e26d-44ae-bfdf-c1878c6a8254"
      },
      "outputs": [],
      "source": [
        "model = create_model()\n",
        "epochs=10\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  epochs=epochs,\n",
        "  steps_per_epoch=1000\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "kcjdUXnrFTbk",
        "outputId": "03a976e2-e334-4d7a-b66f-c9a880e84de2"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['acc'], label='acc')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbVaL-04_GP7",
        "outputId": "9b87f040-c0d1-4898-8a06-66bac24976f7"
      },
      "outputs": [],
      "source": [
        "model.save(\"baslineresnet28\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW__B3wJFTbk",
        "outputId": "17879c92-8762-49a9-ac21-eb8d0a213a06"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(test_ds)\n",
        "print(\"test loss, test acc:\", results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJgQ5ZyYzBiq"
      },
      "outputs": [],
      "source": [
        "def generate_test_summary(model, test_ds):\n",
        "  Y_pred = model.predict(test_ds)\n",
        "  gender_pred = np.where(Y_pred > 0.5, 1, 0)\n",
        "  gender_pred=gender_pred.flatten()\n",
        "  filenames = []\n",
        "  genders = []\n",
        "  ages = []\n",
        "  races = []\n",
        "  for (filename, gender, age, race) in utk_generator(train=False, info=True):\n",
        "    filenames.append(filename)\n",
        "    genders.append(gender)\n",
        "    ages.append(age)\n",
        "    races.append(race)\n",
        "\n",
        "  df = pd.DataFrame({'gender_true': genders,'age_true': ages,'race_true': races,'gender_pred': gender_pred,'files': filenames})\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPlDbFX1tPr_"
      },
      "source": [
        "\n",
        "# Constrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbE2-3AMvCHp"
      },
      "outputs": [],
      "source": [
        "import tensorflow_constrained_optimization as tfco\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8GwNf19vH-R",
        "outputId": "12c54a50-ed73-494f-8290-0a59839d4af2"
      },
      "outputs": [],
      "source": [
        "if tf.__version__ < \"2.0.0\":\n",
        "  tf.compat.v1.enable_eager_execution()\n",
        "  print(\"Eager execution enabled.\")\n",
        "else:\n",
        "  print(\"Eager execution enabled by default.\")\n",
        "\n",
        "print(\"TensorFlow \" + tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HhXg_NBuToK"
      },
      "outputs": [],
      "source": [
        "def save_model(model, subdir):\n",
        "  base_dir = tempfile.mkdtemp(prefix='saved_models')\n",
        "  model_location = os.path.join(base_dir, subdir)\n",
        "  model.save(model_location, save_format='tf')\n",
        "  return model_location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qda2hA4-tPUd"
      },
      "outputs": [],
      "source": [
        "# The batch size is needed to create the input, labels and group tensors.\n",
        "# These tensors are initialized with all 0's. They will eventually be assigned\n",
        "# the batch content to them. A large batch size is chosen so that there are\n",
        "# enough number of \"Young\" and \"Not Young\" examples in each batch.\n",
        "# set_seeds()\n",
        "model_constrained = create_model()\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 28\n",
        "# Create input tensor.\n",
        "input_tensor = tf.Variable(\n",
        "    np.zeros((BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3), dtype=\"float32\"),\n",
        "    name=\"input\")\n",
        "\n",
        "# Create labels and group tensors (assuming both labels and groups are binary).\n",
        "labels_tensor = tf.Variable(\n",
        "    np.zeros(BATCH_SIZE, dtype=\"float32\"), name=\"labels\")\n",
        "groups_tensor = tf.Variable(\n",
        "    np.zeros(BATCH_SIZE, dtype=\"float32\"), name=\"groups\")\n",
        "\n",
        "# Create a function that returns the applied 'model' to the input tensor\n",
        "# and generates constrained predictions.\n",
        "def predictions():\n",
        "  return model_constrained(input_tensor)\n",
        "\n",
        "# Create overall context and subsetted context.\n",
        "# The subsetted context contains subset of examples where group attribute < 1\n",
        "# (i.e. the subset of \"Not Young\" celebrity images).\n",
        "# \"groups_tensor < 1\" is used instead of \"groups_tensor == 0\" as the former\n",
        "# would be a comparison on the tensor value, while the latter would be a\n",
        "# comparison on the Tensor object.\n",
        "context = tfco.rate_context(predictions, labels=lambda:labels_tensor)\n",
        "context_subset = context.subset(lambda:groups_tensor > 0)\n",
        "\n",
        "# Setup list of constraints.\n",
        "# In this notebook, the constraint will just be: FPR to less or equal to 5%.\n",
        "constraints = [tfco.false_negative_rate(context_subset) <= 0.01]\n",
        "\n",
        "# Setup rate minimization problem: minimize overall error rate s.t. constraints.\n",
        "problem = tfco.RateMinimizationProblem(tfco.error_rate(context), constraints)\n",
        "\n",
        "# Create constrained optimizer and obtain train_op.\n",
        "# Separate optimizers are specified for the objective and constraints\n",
        "optimizer = tfco.ProxyLagrangianOptimizerV2(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "      constraint_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "      num_constraints=problem.num_constraints)\n",
        "\n",
        "# A list of all trainable variables is also needed to use TFCO.\n",
        "var_list = (model_constrained.trainable_weights + list(problem.trainable_variables) +\n",
        "            optimizer.trainable_variables())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPV4Hi7pt_Zs",
        "outputId": "209a45a9-40de-454c-dfa1-040078545926"
      },
      "outputs": [],
      "source": [
        "# Obtain train set batches.\n",
        "train_ds_constrained = generate_utk_dataset(include_labels=True, include_groups=True, train=True)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds_constrained = train_ds_constrained.cache().shuffle(1024).repeat().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "NUM_ITERATIONS = 100  # Number of training iterations.\n",
        "SKIP_ITERATIONS = 10  # Print training stats once in this many iterations.\n",
        "\n",
        "# Create temp directory for saving snapshots of models.\n",
        "temp_directory = tempfile.mktemp()\n",
        "os.mkdir(temp_directory)\n",
        "\n",
        "# List of objective and constraints across iterations.\n",
        "objective_list = []\n",
        "violations_list = []\n",
        "\n",
        "# Training iterations.\n",
        "iteration_count = 0\n",
        "for (image, label, group) in train_ds_constrained:\n",
        "  # Assign current batch to input, labels and groups tensors.\n",
        "  input_tensor.assign(image)\n",
        "  labels_tensor.assign(label)\n",
        "  groups_tensor.assign(group)\n",
        "\n",
        "  # Run gradient update.\n",
        "  optimizer.minimize(problem, var_list=var_list)\n",
        "\n",
        "  # Record objective and violations.\n",
        "  objective = problem.objective()\n",
        "  violations = problem.constraints()\n",
        "\n",
        "  sys.stdout.write(\n",
        "      \"\\r Iteration %d: Hinge Loss = %.3f, Max. Constraint Violation = %.3f\"\n",
        "      % (iteration_count + 1, objective, max(violations)))\n",
        "\n",
        "  # Snapshot model once in SKIP_ITERATIONS iterations.\n",
        "  if iteration_count % SKIP_ITERATIONS == 0:\n",
        "    objective_list.append(objective)\n",
        "    violations_list.append(violations)\n",
        "\n",
        "    # Save snapshot of model weights.\n",
        "    model_constrained.save_weights(\n",
        "        temp_directory + \"/celeb_a_constrained_\" +\n",
        "        str(iteration_count / SKIP_ITERATIONS) + \".h5\")\n",
        "\n",
        "  iteration_count += 1\n",
        "  if iteration_count >= NUM_ITERATIONS:\n",
        "    break\n",
        "\n",
        "# Choose best model from recorded iterates and load that model.\n",
        "best_index = tfco.find_best_candidate_index(\n",
        "    np.array(objective_list), np.array(violations_list))\n",
        "\n",
        "model_constrained.load_weights(\n",
        "    temp_directory + \"/celeb_a_constrained_\" + str(best_index) + \".0.h5\")\n",
        "\n",
        "# Remove temp directory.\n",
        "os.system(\"rm -r \" + temp_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUFgJnU5y5TL"
      },
      "outputs": [],
      "source": [
        "df = generate_test_summary(model_constrained, test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf6OKZ8i1ZYA"
      },
      "outputs": [],
      "source": [
        "df.to_csv('constrained.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tfco utk28.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "41a84036aa604917d7497ee77bf20aadd6bf6e037d12a4d13ab984fb2ace29af"
    },
    "kernelspec": {
      "display_name": "Python 3.7.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
