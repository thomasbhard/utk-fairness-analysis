{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "utkface_training_full_weights.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.6 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "41a84036aa604917d7497ee77bf20aadd6bf6e037d12a4d13ab984fb2ace29af"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!unzip UTKFaceFull.zip"
      ],
      "outputs": [],
      "metadata": {
        "id": "aMlmEhY64wwj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "import os\r\n",
        "import glob\r\n",
        "\r\n",
        "import numpy as np \r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "dataset_folder_name = 'dataset/UTKFace'\r\n",
        "outputfile_name = \"df_predictions_all_test.csv\"\r\n",
        "\r\n",
        "\r\n",
        "TRAIN_TEST_SPLIT = 0.7\r\n",
        "IM_WIDTH = IM_HEIGHT = 198\r\n",
        "\r\n",
        "TRAIN_WITH_WEIGTHS = True\r\n",
        "\r\n",
        "\r\n",
        "dataset_dict = {\r\n",
        "    'race_id': {\r\n",
        "        0: 'white', \r\n",
        "        1: 'black', \r\n",
        "        2: 'asian', \r\n",
        "        3: 'indian', \r\n",
        "        4: 'others'\r\n",
        "    },\r\n",
        "    'gender_id': {\r\n",
        "        0: 'male',\r\n",
        "        1: 'female'\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "dataset_dict['gender_alias'] = dict((g, i) for i, g in dataset_dict['gender_id'].items())\r\n",
        "dataset_dict['race_alias'] = dict((g, i) for i, g in dataset_dict['race_id'].items())"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "#weights = {'female': {'white': {'weight': 1.0000859768999493, 'total': 1388}, 'black': {'weight': 1.0000947320909475, 'total': 655}, 'asian': {'weight': 0.9996453453531876, 'total': 568}, 'indian': {'weight': 1.000131283142888, 'total': 523}, 'others': {'weight': 1.0001248417406206, 'total': 298}}, 'male': {'white': {'weight': 1.00007922120209, 'total': 1616}, 'black': {'weight': 0.9998653674745138, 'total': 681}, 'asian': {'weight': 0.999727850814823, 'total': 459}, 'indian': {'weight': 0.9999673852528987, 'total': 637}, 'others': {'weight': 1.0001095340344832, 'total': 215}}}\r\n",
        "weights = {'female': 1.1, 'male': 0.90}\r\n",
        "                  #sample_weights.append(weights[dataset_dict['gender_id'][gender]][dataset_dict['race_id'][race]]['weight'])\r\n",
        "\r\n",
        "def get_weight(gender, age, race):\r\n",
        "    return weights[dataset_dict['gender_id'][gender]]\r\n",
        "\r\n",
        "\r\n",
        "get_weight(0,0,0)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "def parse_dataset(dataset_path, ext='jpg'):\r\n",
        "    \"\"\"\r\n",
        "    Used to extract information about our dataset. It does iterate over all images and return a DataFrame with\r\n",
        "    the data (age, gender and sex) of all files.\r\n",
        "    \"\"\"\r\n",
        "    def parse_info_from_file(path):\r\n",
        "        \"\"\"\r\n",
        "        Parse information from a single file\r\n",
        "        \"\"\"\r\n",
        "        try:\r\n",
        "            filename = os.path.split(path)[1]\r\n",
        "            filename = os.path.splitext(filename)[0]\r\n",
        "            age, gender, race, _ = filename.split('_')\r\n",
        "\r\n",
        "            return int(age), dataset_dict['gender_id'][int(gender)], dataset_dict['race_id'][int(race)]\r\n",
        "        except Exception as ex:\r\n",
        "            return None, None, None\r\n",
        "        \r\n",
        "    files = glob.glob(os.path.join(dataset_path, \"*.%s\" % ext))\r\n",
        "    \r\n",
        "    records = []\r\n",
        "    for file in files:\r\n",
        "        info = parse_info_from_file(file)\r\n",
        "        records.append(info)\r\n",
        "        \r\n",
        "    df = pd.DataFrame(records)\r\n",
        "    df['file'] = files\r\n",
        "    df.columns = ['age', 'gender', 'race', 'file']\r\n",
        "    df = df.dropna()\r\n",
        "    \r\n",
        "    return df\r\n",
        "\r\n",
        "\r\n",
        "df = parse_dataset(dataset_folder_name)\r\n",
        "print(df.head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     age  gender   race                                               file\n",
            "0  100.0    male  white  dataset/UTKFace\\100_0_0_20170112213500903.jpg....\n",
            "1  100.0    male  white  dataset/UTKFace\\100_0_0_20170112215240346.jpg....\n",
            "2  100.0  female  white  dataset/UTKFace\\100_1_0_20170110183726390.jpg....\n",
            "3  100.0  female  white  dataset/UTKFace\\100_1_0_20170112213001988.jpg....\n",
            "4  100.0  female  white  dataset/UTKFace\\100_1_0_20170112213303693.jpg....\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "class UtkFaceDataGenerator():\r\n",
        "    \"\"\"\r\n",
        "    Data generator for the UTKFace dataset. This class should be used when training our Keras multi-output model.\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, df):\r\n",
        "        self.df = df\r\n",
        "        \r\n",
        "    def generate_split_indexes(self):\r\n",
        "        p = np.random.RandomState(seed=42).permutation(len(self.df))\r\n",
        "        train_up_to = int(len(self.df) * TRAIN_TEST_SPLIT)\r\n",
        "        train_idx = p[:train_up_to]\r\n",
        "        test_idx = p[train_up_to:]\r\n",
        "\r\n",
        "        train_up_to = int(train_up_to * TRAIN_TEST_SPLIT)\r\n",
        "        train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:]\r\n",
        "        \r\n",
        "        # converts alias to id\r\n",
        "        self.df['gender_id'] = self.df['gender'].map(lambda gender: dataset_dict['gender_alias'][gender])\r\n",
        "        self.df['race_id'] = self.df['race'].map(lambda race: dataset_dict['race_alias'][race])\r\n",
        "\r\n",
        "        self.max_age = self.df['age'].max()\r\n",
        "        \r\n",
        "        return train_idx, valid_idx, test_idx\r\n",
        "    \r\n",
        "    def preprocess_image(self, img_path):\r\n",
        "        \"\"\"\r\n",
        "        Used to perform some minor preprocessing on the image before inputting into the network.\r\n",
        "        \"\"\"\r\n",
        "        im = Image.open(img_path)\r\n",
        "        im = im.resize((IM_WIDTH, IM_HEIGHT))\r\n",
        "        im = np.array(im) / 255.0\r\n",
        "        \r\n",
        "        return im\r\n",
        "        \r\n",
        "    def generate_images(self, image_idx, is_training, batch_size=16, include_weights=False, include_info=False):\r\n",
        "        \"\"\"\r\n",
        "        Used to generate a batch with images when training/testing/validating our Keras model.\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        # arrays to store our batched data\r\n",
        "        images, ages, races, genders = [], [], [], []\r\n",
        "\r\n",
        "        # weights\r\n",
        "        sample_weights = []\r\n",
        "        files = []\r\n",
        "        while True:\r\n",
        "            for idx in image_idx:\r\n",
        "                person = self.df.iloc[idx]\r\n",
        "\r\n",
        "                file = person['file']\r\n",
        "\r\n",
        "                gender = person['gender_id']\r\n",
        "                genders.append(to_categorical(gender, len(dataset_dict['gender_id'])))\r\n",
        "\r\n",
        "                im = self.preprocess_image(file)\r\n",
        "                images.append(im)\r\n",
        "\r\n",
        "                age = person['age']\r\n",
        "                race = person['race_id']\r\n",
        "\r\n",
        "\r\n",
        "                if include_info:\r\n",
        "                  ages.append(age / self.max_age)\r\n",
        "                  races.append(to_categorical(race, len(dataset_dict['race_id'])))\r\n",
        "\r\n",
        "                  files.append(file)\r\n",
        "                \r\n",
        "                \r\n",
        "                if include_weights:\r\n",
        "                  sample_weights.append(get_weight(gender, age, race))\r\n",
        "\r\n",
        "                \r\n",
        "                # yielding condition\r\n",
        "                if len(images) >= batch_size:\r\n",
        "                    if include_info and include_weights:\r\n",
        "                      yield np.array(images), np.array(genders), np.array(sample_weights), np.array(files), np.array(ages), np.array(races)\r\n",
        "                    \r\n",
        "                    elif include_info:\r\n",
        "                      yield np.array(images), np.array(genders), np.array(files), np.array(age), np.array(race)\r\n",
        "\r\n",
        "                    elif include_weights:\r\n",
        "                      yield np.array(images), np.array(genders), np.array(sample_weights)\r\n",
        "\r\n",
        "                    else:\r\n",
        "                      yield np.array(images), np.array(genders)\r\n",
        "\r\n",
        "                    images, ages, races, genders, sample_weights, files = [], [], [], [], [], []\r\n",
        "                    \r\n",
        "            if not is_training:\r\n",
        "                break\r\n",
        "                \r\n",
        "data_generator = UtkFaceDataGenerator(df)\r\n",
        "train_idx, valid_idx, test_idx = data_generator.generate_split_indexes()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "from keras.models import Model\r\n",
        "from keras.layers import BatchNormalization\r\n",
        "from keras.layers.convolutional import Conv2D\r\n",
        "from keras.layers.convolutional import MaxPooling2D\r\n",
        "from keras.layers.core import Activation\r\n",
        "from keras.layers.core import Dropout\r\n",
        "from keras.layers.core import Lambda\r\n",
        "from keras.layers.core import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import Input\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "class UtkMultiOutputModel():\r\n",
        "    \"\"\"\r\n",
        "    Used to generate our multi-output model. This CNN contains three branches, one for age, other for \r\n",
        "    sex and another for race. Each branch contains a sequence of Convolutional Layers that is defined\r\n",
        "    on the make_default_hidden_layers method.\r\n",
        "    \"\"\"\r\n",
        "    def make_default_hidden_layers(self, inputs):\r\n",
        "        \"\"\"\r\n",
        "        Used to generate a default set of hidden layers. The structure used in this network is defined as:\r\n",
        "        \r\n",
        "        Conv2D -> BatchNormalization -> Pooling -> Dropout\r\n",
        "        \"\"\"\r\n",
        "        x = Conv2D(16, (3, 3), padding=\"same\")(inputs)\r\n",
        "        x = Activation(\"relu\")(x)\r\n",
        "        x = BatchNormalization(axis=-1)(x)\r\n",
        "        x = MaxPooling2D(pool_size=(3, 3))(x)\r\n",
        "        x = Dropout(0.25)(x)\r\n",
        "\r\n",
        "        x = Conv2D(32, (3, 3), padding=\"same\")(x)\r\n",
        "        x = Activation(\"relu\")(x)\r\n",
        "        x = BatchNormalization(axis=-1)(x)\r\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\r\n",
        "        x = Dropout(0.25)(x)\r\n",
        "\r\n",
        "        x = Conv2D(32, (3, 3), padding=\"same\")(x)\r\n",
        "        x = Activation(\"relu\")(x)\r\n",
        "        x = BatchNormalization(axis=-1)(x)\r\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\r\n",
        "        x = Dropout(0.25)(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "    def build_gender_branch(self, inputs, num_genders=2):\r\n",
        "        \"\"\"\r\n",
        "        Used to build the gender branch of our face recognition network.\r\n",
        "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \r\n",
        "        followed by the Dense output layer.\r\n",
        "        \"\"\"\r\n",
        "        x = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs)\r\n",
        "\r\n",
        "        x = self.make_default_hidden_layers(inputs)\r\n",
        "\r\n",
        "        x = Flatten()(x)\r\n",
        "        x = Dense(128)(x)\r\n",
        "        x = Activation(\"relu\")(x)\r\n",
        "        x = BatchNormalization()(x)\r\n",
        "        x = Dropout(0.5)(x)\r\n",
        "        x = Dense(num_genders)(x)\r\n",
        "        x = Activation(\"sigmoid\", name=\"gender_output\")(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "    def assemble_full_model(self, width, height):\r\n",
        "        \"\"\"\r\n",
        "        Used to assemble our multi-output model CNN.\r\n",
        "        \"\"\"\r\n",
        "        input_shape = (height, width, 3)\r\n",
        "\r\n",
        "        inputs = Input(shape=input_shape)\r\n",
        "\r\n",
        "        gender_branch = self.build_gender_branch(inputs)\r\n",
        "\r\n",
        "        model = Model(inputs=inputs,\r\n",
        "                     outputs = gender_branch,\r\n",
        "                     name=\"face_net\")\r\n",
        "\r\n",
        "        return model\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "model = UtkMultiOutputModel().assemble_full_model(IM_WIDTH, IM_HEIGHT)\r\n",
        "\r\n",
        "model.summary()\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"face_net\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 198, 198, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 198, 198, 16)      448       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 198, 198, 16)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 198, 198, 16)      64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 66, 66, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 66, 66, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 66, 66, 32)        4640      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 66, 66, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 66, 66, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 33, 33, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 33, 33, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 33, 33, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 33, 33, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 33, 33, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               1048704   \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 258       \n",
            "_________________________________________________________________\n",
            "gender_output (Activation)   (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 1,064,130\n",
            "Trainable params: 1,063,714\n",
            "Non-trainable params: 416\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ReM53Ld46Gr",
        "outputId": "1419290b-859d-47f6-ea70-32193e98081c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "from tensorflow.keras.optimizers import Adam\r\n",
        "\r\n",
        "\r\n",
        "init_lr = 1e-4\r\n",
        "epochs = 1\r\n",
        "\r\n",
        "opt = Adam(lr=init_lr, decay=init_lr / epochs)\r\n",
        "\r\n",
        "model.compile(optimizer=opt, \r\n",
        "              loss='binary_crossentropy',\r\n",
        "              loss_weights= 0.1,\r\n",
        "              metrics= 'accuracy')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "from keras.callbacks import ModelCheckpoint\r\n",
        "\r\n",
        "batch_size = 32\r\n",
        "valid_batch_size = 32\r\n",
        "train_gen = data_generator.generate_images(train_idx, is_training=True, batch_size=batch_size, include_weights=TRAIN_WITH_WEIGTHS)\r\n",
        "valid_gen = data_generator.generate_images(valid_idx, is_training=True, batch_size=valid_batch_size, include_weights=TRAIN_WITH_WEIGTHS)\r\n",
        "\r\n",
        "callbacks = [\r\n",
        "    ModelCheckpoint(\"./model_checkpoint\", monitor='val_loss')\r\n",
        "]\r\n",
        "\r\n",
        "history = model.fit(train_gen,\r\n",
        "                    steps_per_epoch=len(train_idx)//batch_size,\r\n",
        "                    epochs=epochs,\r\n",
        "                    callbacks=callbacks,\r\n",
        "                    validation_data=valid_gen,\r\n",
        "                    validation_steps=len(valid_idx)//valid_batch_size)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "362/362 [==============================] - 651s 2s/step - loss: 0.0749 - accuracy: 0.6890 - val_loss: 0.1093 - val_accuracy: 0.5462\n",
            "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXmta98h5AeB",
        "outputId": "a2d836a6-3562-4511-d80c-2ab3d6943139"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'val'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "id": "c2ny3d36AaDg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "0b1faee9-c49c-4287-8cc4-a8ee11eec2b7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "test_batch_size = 128\r\n",
        "test_generator = data_generator.generate_images(test_idx, is_training=False, batch_size=test_batch_size)\r\n",
        "gender_pred = model.predict_generator(test_generator, steps=len(test_idx)//test_batch_size)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Fij3EaUYKK",
        "outputId": "210a415c-a95b-46a7-ba51-af4d3f8e7163"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "test_generator = data_generator.generate_images(test_idx, is_training=False, batch_size=test_batch_size, include_weights=True, include_info=True)\r\n",
        "samples = 0\r\n",
        "images, age_true, race_true, gender_true, sample_weights, files = [], [], [], [], [], []\r\n",
        "for test_batch in test_generator:\r\n",
        "    image = test_batch[0]\r\n",
        "    \r\n",
        "    images.extend(image)\r\n",
        "    gender_true.extend(test_batch[1])\r\n",
        "\r\n",
        "    sample_weights.extend(test_batch[2])\r\n",
        "    files.extend(test_batch[3])\r\n",
        "    age_true.extend(test_batch[4])\r\n",
        "    race_true.extend(test_batch[5])\r\n",
        "    \r\n",
        "age_true = np.array(age_true)\r\n",
        "race_true = np.array(race_true)\r\n",
        "gender_true = np.array(gender_true)\r\n",
        "\r\n",
        "race_true, gender_true = race_true.argmax(axis=-1), gender_true.argmax(axis=-1)\r\n",
        "gender_pred = gender_pred.argmax(axis=-1)\r\n",
        "\r\n",
        "age_true = age_true * data_generator.max_age"
      ],
      "outputs": [],
      "metadata": {
        "id": "gyEiqloIU88y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "cr_gender = classification_report(gender_true, gender_pred, target_names=dataset_dict['gender_alias'].keys())\r\n",
        "print(cr_gender)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        male       0.53      1.00      0.70      3691\n",
            "      female       0.97      0.04      0.08      3349\n",
            "\n",
            "    accuracy                           0.54      7040\n",
            "   macro avg       0.75      0.52      0.39      7040\n",
            "weighted avg       0.74      0.54      0.40      7040\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOdJh_kMVrka",
        "outputId": "d700d1ae-d740-439e-c504-6b15b2005f27"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "df_prediction = pd.DataFrame({'age_true': age_true, 'race_true': race_true, 'gender_true': gender_true, 'gender_pred': gender_pred})\r\n",
        "df_prediction = df_prediction.round(0).astype(int)\r\n",
        "df_prediction['weights'] = sample_weights\r\n",
        "df_prediction['files'] = files\r\n",
        "df_prediction.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age_true</th>\n",
              "      <th>race_true</th>\n",
              "      <th>gender_true</th>\n",
              "      <th>gender_pred</th>\n",
              "      <th>weights</th>\n",
              "      <th>files</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>dataset/UTKFace\\29_0_4_20170103235840396.jpg.c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>dataset/UTKFace\\60_1_1_20170113185112295.jpg.c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>dataset/UTKFace\\5_1_4_20170116232419298.jpg.ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>dataset/UTKFace\\26_1_3_20170119180953860.jpg.c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>dataset/UTKFace\\16_0_0_20170110232450588.jpg.c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age_true  race_true  gender_true  gender_pred  weights  \\\n",
              "0        29          4            0            0      0.9   \n",
              "1        60          1            1            0      1.1   \n",
              "2         5          4            1            0      1.1   \n",
              "3        26          3            1            0      1.1   \n",
              "4        16          0            0            0      0.9   \n",
              "\n",
              "                                               files  \n",
              "0  dataset/UTKFace\\29_0_4_20170103235840396.jpg.c...  \n",
              "1  dataset/UTKFace\\60_1_1_20170113185112295.jpg.c...  \n",
              "2  dataset/UTKFace\\5_1_4_20170116232419298.jpg.ch...  \n",
              "3  dataset/UTKFace\\26_1_3_20170119180953860.jpg.c...  \n",
              "4  dataset/UTKFace\\16_0_0_20170110232450588.jpg.c...  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "gZ3Aw7PGXzxS",
        "outputId": "9a3158d8-85a4-4a47-a9d7-203b51437ddd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "source": [
        "import time\r\n",
        "\r\n",
        "outfilebase = time.strftime(\"%Y%m%d-%H%M%S\", time.gmtime(time.time()))\r\n",
        "\r\n",
        "outputfile = outfilebase + '_' + outputfile_name\r\n",
        "\r\n",
        "df_prediction.to_csv(outputfile)"
      ],
      "outputs": [],
      "metadata": {
        "id": "niYf_nJcYv-B"
      }
    }
  ]
}