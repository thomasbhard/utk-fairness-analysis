{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "utkface_training_gender.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training gender, race, and age classifier for UTKFace dataset\r\n",
        "\r\n",
        "This code is based on [Rodrigo Bressan](https://github.com/rodrigobressan/keras-multi-output-model-utk-face)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!unzip UTKFaceFull.zip"
      ],
      "outputs": [],
      "metadata": {
        "id": "aMlmEhY64wwj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "import glob\r\n",
        "\r\n",
        "import numpy as np \r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "dataset_folder_name = 'UTKFace'\r\n",
        "\r\n",
        "TRAIN_TEST_SPLIT = 0.7\r\n",
        "IM_WIDTH = IM_HEIGHT = 198\r\n",
        "\r\n",
        "dataset_dict = {\r\n",
        "    'race_id': {\r\n",
        "        0: 'white', \r\n",
        "        1: 'black', \r\n",
        "        2: 'asian', \r\n",
        "        3: 'indian', \r\n",
        "        4: 'others'\r\n",
        "    },\r\n",
        "    'gender_id': {\r\n",
        "        0: 'male',\r\n",
        "        1: 'female'\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "dataset_dict['gender_alias'] = dict((g, i) for i, g in dataset_dict['gender_id'].items())\r\n",
        "dataset_dict['race_alias'] = dict((g, i) for i, g in dataset_dict['race_id'].items())\r\n",
        "\r\n",
        "\r\n",
        "def parse_dataset(dataset_path, ext='jpg'):\r\n",
        "    \"\"\"\r\n",
        "    Used to extract information about our dataset. It does iterate over all images and return a DataFrame with\r\n",
        "    the data (age, gender and sex) of all files.\r\n",
        "    \"\"\"\r\n",
        "    def parse_info_from_file(path):\r\n",
        "        \"\"\"\r\n",
        "        Parse information from a single file\r\n",
        "        \"\"\"\r\n",
        "        try:\r\n",
        "            filename = os.path.split(path)[1]\r\n",
        "            filename = os.path.splitext(filename)[0]\r\n",
        "            age, gender, race, _ = filename.split('_')\r\n",
        "\r\n",
        "            return int(age), dataset_dict['gender_id'][int(gender)], dataset_dict['race_id'][int(race)]\r\n",
        "        except Exception as ex:\r\n",
        "            return None, None, None\r\n",
        "        \r\n",
        "    files = glob.glob(os.path.join(dataset_path, \"*.%s\" % ext))\r\n",
        "    \r\n",
        "    records = []\r\n",
        "    for file in files:\r\n",
        "        info = parse_info_from_file(file)\r\n",
        "        records.append(info)\r\n",
        "        \r\n",
        "    df = pd.DataFrame(records)\r\n",
        "    df['file'] = files\r\n",
        "    df.columns = ['age', 'gender', 'race', 'file']\r\n",
        "    df = df.dropna()\r\n",
        "    \r\n",
        "    return df\r\n",
        "\r\n",
        "\r\n",
        "df = parse_dataset(dataset_folder_name)\r\n",
        "print(df.head())\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class UtkFaceDataGenerator():\r\n",
        "    \"\"\"\r\n",
        "    Data generator for the UTKFace dataset. This class should be used when training our Keras multi-output model.\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, df):\r\n",
        "        self.df = df\r\n",
        "        \r\n",
        "    def generate_split_indexes(self):\r\n",
        "        p = np.random.permutation(len(self.df))\r\n",
        "        train_up_to = int(len(self.df) * TRAIN_TEST_SPLIT)\r\n",
        "        train_idx = p[:train_up_to]\r\n",
        "        test_idx = p[train_up_to:]\r\n",
        "\r\n",
        "        train_up_to = int(train_up_to * TRAIN_TEST_SPLIT)\r\n",
        "        train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:]\r\n",
        "        \r\n",
        "        # converts alias to id\r\n",
        "        self.df['gender_id'] = self.df['gender'].map(lambda gender: dataset_dict['gender_alias'][gender])\r\n",
        "        self.df['race_id'] = self.df['race'].map(lambda race: dataset_dict['race_alias'][race])\r\n",
        "\r\n",
        "        self.max_age = self.df['age'].max()\r\n",
        "        \r\n",
        "        return train_idx, valid_idx, test_idx\r\n",
        "    \r\n",
        "    def preprocess_image(self, img_path):\r\n",
        "        \"\"\"\r\n",
        "        Used to perform some minor preprocessing on the image before inputting into the network.\r\n",
        "        \"\"\"\r\n",
        "        im = Image.open(img_path)\r\n",
        "        im = im.resize((IM_WIDTH, IM_HEIGHT))\r\n",
        "        im = np.array(im) / 255.0\r\n",
        "        \r\n",
        "        return im\r\n",
        "        \r\n",
        "    def generate_images(self, image_idx, is_training, batch_size=16):\r\n",
        "        \"\"\"\r\n",
        "        Used to generate a batch with images when training/testing/validating our Keras model.\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        # arrays to store our batched data\r\n",
        "        images, ages, races, genders = [], [], [], []\r\n",
        "        while True:\r\n",
        "            for idx in image_idx:\r\n",
        "                person = self.df.iloc[idx]\r\n",
        "                \r\n",
        "                age = person['age']\r\n",
        "                race = person['race_id']\r\n",
        "                gender = person['gender_id']\r\n",
        "                file = person['file']\r\n",
        "                \r\n",
        "                im = self.preprocess_image(file)\r\n",
        "                \r\n",
        "                ages.append(age / self.max_age)\r\n",
        "                races.append(to_categorical(race, len(dataset_dict['race_id'])))\r\n",
        "                genders.append(to_categorical(gender, len(dataset_dict['gender_id'])))\r\n",
        "                images.append(im)\r\n",
        "                \r\n",
        "                # yielding condition\r\n",
        "                if len(images) >= batch_size:\r\n",
        "                    yield np.array(images), [np.array(ages), np.array(races), np.array(genders)]\r\n",
        "                    images, ages, races, genders = [], [], [], []\r\n",
        "                    \r\n",
        "            if not is_training:\r\n",
        "                break\r\n",
        "        \r\n",
        "    def generate_images_gender(self, image_idx, is_training, batch_size=16):\r\n",
        "        \"\"\"\r\n",
        "        Used to generate a batch with images when training/testing/validating our Keras model.\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        # arrays to store our batched data\r\n",
        "        images, genders = [], []\r\n",
        "        while True:\r\n",
        "            for idx in image_idx:\r\n",
        "                person = self.df.iloc[idx]\r\n",
        "                \r\n",
        "                gender = person['gender_id']\r\n",
        "                file = person['file']\r\n",
        "                \r\n",
        "                im = self.preprocess_image(file)\r\n",
        "                \r\n",
        "                genders.append(to_categorical(gender, len(dataset_dict['gender_id'])))\r\n",
        "                images.append(im)\r\n",
        "                \r\n",
        "                # yielding condition\r\n",
        "                if len(images) >= batch_size:\r\n",
        "                    yield np.array(images), np.array(genders)\r\n",
        "                    images, genders = [], []\r\n",
        "                    \r\n",
        "            if not is_training:\r\n",
        "                break\r\n",
        "\r\n",
        "       \r\n",
        "data_generator = UtkFaceDataGenerator(df)\r\n",
        "train_idx, valid_idx, test_idx = data_generator.generate_split_indexes()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.layers.convolutional import Conv2D\r\n",
        "from keras.layers.convolutional import MaxPooling2D\r\n",
        "from keras.layers.core import Activation\r\n",
        "from keras.layers.core import Dropout\r\n",
        "from keras.layers.core import Lambda\r\n",
        "from keras.layers.core import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import Input\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "class UtkMultiOutputModel():\r\n",
        "    \"\"\"\r\n",
        "    Used to generate our multi-output model. This CNN contains three branches, one for age, other for \r\n",
        "    sex and another for race. Each branch contains a sequence of Convolutional Layers that is defined\r\n",
        "    on the make_default_hidden_layers method.\r\n",
        "    \"\"\"\r\n",
        "    def make_default_hidden_layers(self, inputs):\r\n",
        "        \"\"\"\r\n",
        "        Used to generate a default set of hidden layers. The structure used in this network is defined as:\r\n",
        "        \r\n",
        "        Conv2D -> BatchNormalization -> Pooling -> Dropout\r\n",
        "        \"\"\"\r\n",
        "        x = Conv2D(16, (3, 3), padding=\"same\")(inputs)\r\n",
        "        x = Activation(\"relu\")(x)\r\n",
        "        x = BatchNormalization(axis=-1)(x)\r\n",
        "        x = MaxPooling2D(pool_size=(3, 3))(x)\r\n",
        "        x = Dropout(0.25)(x)\r\n",
        "\r\n",
        "        x = Conv2D(32, (3, 3), padding=\"same\")(x)\r\n",
        "        x = Activation(\"relu\")(x)\r\n",
        "        x = BatchNormalization(axis=-1)(x)\r\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\r\n",
        "        x = Dropout(0.25)(x)\r\n",
        "\r\n",
        "        x = Conv2D(32, (3, 3), padding=\"same\")(x)\r\n",
        "        x = Activation(\"relu\")(x)\r\n",
        "        x = BatchNormalization(axis=-1)(x)\r\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\r\n",
        "        x = Dropout(0.25)(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "    def build_race_branch(self, inputs, num_races):\r\n",
        "        \"\"\"\r\n",
        "        Used to build the race branch of our face recognition network.\r\n",
        "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \r\n",
        "        followed by the Dense output layer.\r\n",
        "        \"\"\"\r\n",
        "        x = self.make_default_hidden_layers(inputs)\r\n",
        "\r\n",
        "        x = Flatten()(x)\r\n",
        "        x = Dense(128)(x)\r\n",
        "        x = Activation(\"relu\")(x)\r\n",
        "        x = BatchNormalization()(x)\r\n",
        "        x = Dropout(0.5)(x)\r\n",
        "        x = Dense(num_races)(x)\r\n",
        "        x = Activation(\"softmax\", name=\"race_output\")(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "    def build_gender_branch(self, inputs, num_genders=2):\r\n",
        "        \"\"\"\r\n",
        "        Used to build the gender branch of our face recognition network.\r\n",
        "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \r\n",
        "        followed by the Dense output layer.\r\n",
        "        \"\"\"\r\n",
        "        x = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs)\r\n",
        "\r\n",
        "        x = self.make_default_hidden_layers(inputs)\r\n",
        "\r\n",
        "        x = Flatten()(x)\r\n",
        "        x = Dense(128)(x)\r\n",
        "        x = Activation(\"relu\")(x)\r\n",
        "        x = BatchNormalization()(x)\r\n",
        "        x = Dropout(0.5)(x)\r\n",
        "        x = Dense(num_genders)(x)\r\n",
        "        x = Activation(\"sigmoid\", name=\"gender_output\")(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "    def build_age_branch(self, inputs):   \r\n",
        "        \"\"\"\r\n",
        "        Used to build the age branch of our face recognition network.\r\n",
        "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \r\n",
        "        followed by the Dense output layer.\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        x = self.make_default_hidden_layers(inputs)\r\n",
        "\r\n",
        "        x = Flatten()(x)\r\n",
        "        x = Dense(128)(x)\r\n",
        "        x = Activation(\"relu\")(x)\r\n",
        "        x = BatchNormalization()(x)\r\n",
        "        x = Dropout(0.5)(x)\r\n",
        "        x = Dense(1)(x)\r\n",
        "        x = Activation(\"linear\", name=\"age_output\")(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "    def assemble_full_model(self, width, height, num_races):\r\n",
        "        \"\"\"\r\n",
        "        Used to assemble our multi-output model CNN.\r\n",
        "        \"\"\"\r\n",
        "        input_shape = (height, width, 3)\r\n",
        "\r\n",
        "        inputs = Input(shape=input_shape)\r\n",
        "\r\n",
        "        age_branch = self.build_age_branch(inputs)\r\n",
        "        race_branch = self.build_race_branch(inputs, num_races)\r\n",
        "        gender_branch = self.build_gender_branch(inputs)\r\n",
        "\r\n",
        "        model = Model(inputs=inputs,\r\n",
        "                     outputs = [age_branch, race_branch, gender_branch],\r\n",
        "                     name=\"face_net\")\r\n",
        "\r\n",
        "        return model\r\n",
        "\r\n",
        "    def assemble_gender_only(self, width, height):\r\n",
        "        input_shape = (height, width, 3)\r\n",
        "\r\n",
        "        inputs = Input(shape=input_shape)\r\n",
        "\r\n",
        "        gender_branch = self.build_gender_branch(inputs)\r\n",
        "\r\n",
        "        model = Model(inputs=inputs,\r\n",
        "                     outputs = gender_branch,\r\n",
        "                     name=\"face_net\")\r\n",
        "\r\n",
        "        return model\r\n",
        "\r\n",
        "    \r\n",
        "model = UtkMultiOutputModel().assemble_gender_only(IM_WIDTH, IM_HEIGHT)\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "from keras.optimizers import Adam\r\n",
        "\r\n",
        "init_lr = 1e-4\r\n",
        "epochs = 50\r\n",
        "\r\n",
        "opt = Adam(learning_rate=init_lr, decay=init_lr / epochs)\r\n",
        "\r\n",
        "model.compile(optimizer=opt, \r\n",
        "              loss='binary_crossentropy',\r\n",
        "              loss_weights=0.1,\r\n",
        "              metrics='accuracy')\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    age  gender   race                                           file\n",
            "0  16.0  female  white  UTKFace/16_1_0_20170109214757287.jpg.chip.jpg\n",
            "1  23.0    male  asian  UTKFace/23_0_2_20170116172945780.jpg.chip.jpg\n",
            "2  40.0    male  white  UTKFace/40_0_0_20170117182850349.jpg.chip.jpg\n",
            "3   1.0  female  white   UTKFace/1_1_0_20170109191449860.jpg.chip.jpg\n",
            "4  28.0    male  asian  UTKFace/28_0_2_20170104021218340.jpg.chip.jpg\n",
            "Model: \"face_net\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 198, 198, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 198, 198, 16)      448       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 198, 198, 16)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 198, 198, 16)      64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 66, 66, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 66, 66, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 66, 66, 32)        4640      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 66, 66, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 66, 66, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 33, 33, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 33, 33, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 33, 33, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 33, 33, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 33, 33, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               1048704   \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 258       \n",
            "_________________________________________________________________\n",
            "gender_output (Activation)   (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 1,064,130\n",
            "Trainable params: 1,063,714\n",
            "Non-trainable params: 416\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ReM53Ld46Gr",
        "outputId": "b6286f1c-31d1-4a27-8a81-3edc7ae72c7f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from keras.callbacks import ModelCheckpoint\r\n",
        "\r\n",
        "batch_size = 32\r\n",
        "valid_batch_size = 32\r\n",
        "train_gen = data_generator.generate_images_gender(train_idx, is_training=True, batch_size=batch_size)\r\n",
        "valid_gen = data_generator.generate_images_gender(valid_idx, is_training=True, batch_size=valid_batch_size)\r\n",
        "\r\n",
        "callbacks = [\r\n",
        "    ModelCheckpoint(\"./model_checkpoint\", monitor='val_loss')\r\n",
        "]\r\n",
        "\r\n",
        "history = model.fit(train_gen,\r\n",
        "                    steps_per_epoch=len(train_idx)//batch_size,\r\n",
        "                    epochs=epochs,\r\n",
        "                    callbacks=callbacks,\r\n",
        "                    validation_data=valid_gen,\r\n",
        "                    validation_steps=len(valid_idx)//valid_batch_size)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "362/362 [==============================] - 108s 165ms/step - loss: 0.0757 - accuracy: 0.6810 - val_loss: 0.0553 - val_accuracy: 0.7141\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 2/50\n",
            "362/362 [==============================] - 59s 163ms/step - loss: 0.0507 - accuracy: 0.7969 - val_loss: 0.1152 - val_accuracy: 0.7069\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 3/50\n",
            "362/362 [==============================] - 59s 163ms/step - loss: 0.0459 - accuracy: 0.8146 - val_loss: 0.1013 - val_accuracy: 0.7419\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 4/50\n",
            "362/362 [==============================] - 59s 164ms/step - loss: 0.0404 - accuracy: 0.8415 - val_loss: 0.1148 - val_accuracy: 0.7560\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 5/50\n",
            "362/362 [==============================] - 59s 163ms/step - loss: 0.0378 - accuracy: 0.8441 - val_loss: 0.0981 - val_accuracy: 0.7792\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 6/50\n",
            "362/362 [==============================] - 59s 164ms/step - loss: 0.0362 - accuracy: 0.8485 - val_loss: 0.1430 - val_accuracy: 0.7887\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 7/50\n",
            "362/362 [==============================] - 59s 164ms/step - loss: 0.0340 - accuracy: 0.8617 - val_loss: 0.0919 - val_accuracy: 0.8214\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 8/50\n",
            "362/362 [==============================] - 58s 161ms/step - loss: 0.0329 - accuracy: 0.8623 - val_loss: 0.0720 - val_accuracy: 0.8407\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 9/50\n",
            "362/362 [==============================] - 59s 164ms/step - loss: 0.0328 - accuracy: 0.8671 - val_loss: 0.0808 - val_accuracy: 0.8304\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 10/50\n",
            "362/362 [==============================] - 59s 163ms/step - loss: 0.0310 - accuracy: 0.8720 - val_loss: 0.0563 - val_accuracy: 0.8522\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 11/50\n",
            "362/362 [==============================] - 58s 161ms/step - loss: 0.0296 - accuracy: 0.8814 - val_loss: 0.0672 - val_accuracy: 0.8506\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 12/50\n",
            "362/362 [==============================] - 59s 164ms/step - loss: 0.0288 - accuracy: 0.8824 - val_loss: 0.0546 - val_accuracy: 0.8546\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 13/50\n",
            "362/362 [==============================] - 60s 165ms/step - loss: 0.0270 - accuracy: 0.8876 - val_loss: 0.0499 - val_accuracy: 0.8595\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 14/50\n",
            "362/362 [==============================] - 59s 163ms/step - loss: 0.0264 - accuracy: 0.8914 - val_loss: 0.0415 - val_accuracy: 0.8742\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 15/50\n",
            "362/362 [==============================] - 60s 166ms/step - loss: 0.0263 - accuracy: 0.8870 - val_loss: 0.0397 - val_accuracy: 0.8720\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 16/50\n",
            "362/362 [==============================] - 60s 166ms/step - loss: 0.0256 - accuracy: 0.8929 - val_loss: 0.0355 - val_accuracy: 0.8734\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 17/50\n",
            "362/362 [==============================] - 59s 164ms/step - loss: 0.0247 - accuracy: 0.8972 - val_loss: 0.0342 - val_accuracy: 0.8806\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 18/50\n",
            "362/362 [==============================] - 59s 164ms/step - loss: 0.0240 - accuracy: 0.9001 - val_loss: 0.0365 - val_accuracy: 0.8794\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 19/50\n",
            "362/362 [==============================] - 60s 166ms/step - loss: 0.0239 - accuracy: 0.8975 - val_loss: 0.0317 - val_accuracy: 0.8835\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 20/50\n",
            "362/362 [==============================] - 59s 163ms/step - loss: 0.0236 - accuracy: 0.9027 - val_loss: 0.0347 - val_accuracy: 0.8754\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 21/50\n",
            "362/362 [==============================] - 59s 163ms/step - loss: 0.0229 - accuracy: 0.9120 - val_loss: 0.0309 - val_accuracy: 0.8819\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 22/50\n",
            "362/362 [==============================] - 60s 165ms/step - loss: 0.0221 - accuracy: 0.9113 - val_loss: 0.0330 - val_accuracy: 0.8819\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 23/50\n",
            "362/362 [==============================] - 59s 164ms/step - loss: 0.0214 - accuracy: 0.9134 - val_loss: 0.0309 - val_accuracy: 0.8857\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 24/50\n",
            "362/362 [==============================] - 58s 162ms/step - loss: 0.0206 - accuracy: 0.9140 - val_loss: 0.0305 - val_accuracy: 0.8887\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 25/50\n",
            "362/362 [==============================] - 58s 161ms/step - loss: 0.0194 - accuracy: 0.9210 - val_loss: 0.0288 - val_accuracy: 0.8861\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 26/50\n",
            "362/362 [==============================] - 58s 161ms/step - loss: 0.0199 - accuracy: 0.9230 - val_loss: 0.0306 - val_accuracy: 0.8895\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 27/50\n",
            "362/362 [==============================] - 58s 160ms/step - loss: 0.0194 - accuracy: 0.9249 - val_loss: 0.0286 - val_accuracy: 0.8857\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 28/50\n",
            "362/362 [==============================] - 57s 158ms/step - loss: 0.0181 - accuracy: 0.9268 - val_loss: 0.0354 - val_accuracy: 0.8841\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 29/50\n",
            "362/362 [==============================] - 58s 160ms/step - loss: 0.0184 - accuracy: 0.9289 - val_loss: 0.0325 - val_accuracy: 0.8819\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 30/50\n",
            "362/362 [==============================] - 58s 160ms/step - loss: 0.0179 - accuracy: 0.9265 - val_loss: 0.0305 - val_accuracy: 0.8895\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 31/50\n",
            "362/362 [==============================] - 58s 160ms/step - loss: 0.0173 - accuracy: 0.9332 - val_loss: 0.0302 - val_accuracy: 0.8935\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 32/50\n",
            "362/362 [==============================] - 59s 162ms/step - loss: 0.0178 - accuracy: 0.9294 - val_loss: 0.0310 - val_accuracy: 0.8913\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 33/50\n",
            "362/362 [==============================] - 58s 161ms/step - loss: 0.0165 - accuracy: 0.9334 - val_loss: 0.0283 - val_accuracy: 0.8944\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 34/50\n",
            "362/362 [==============================] - 58s 160ms/step - loss: 0.0158 - accuracy: 0.9366 - val_loss: 0.0314 - val_accuracy: 0.8931\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 35/50\n",
            "362/362 [==============================] - 59s 163ms/step - loss: 0.0150 - accuracy: 0.9416 - val_loss: 0.0287 - val_accuracy: 0.8919\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 36/50\n",
            "362/362 [==============================] - 59s 163ms/step - loss: 0.0149 - accuracy: 0.9405 - val_loss: 0.0278 - val_accuracy: 0.8927\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 37/50\n",
            "362/362 [==============================] - 58s 161ms/step - loss: 0.0146 - accuracy: 0.9416 - val_loss: 0.0290 - val_accuracy: 0.8956\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 38/50\n",
            "362/362 [==============================] - 58s 161ms/step - loss: 0.0141 - accuracy: 0.9460 - val_loss: 0.0298 - val_accuracy: 0.8907\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 39/50\n",
            "362/362 [==============================] - 57s 159ms/step - loss: 0.0136 - accuracy: 0.9484 - val_loss: 0.0316 - val_accuracy: 0.8915\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 40/50\n",
            "362/362 [==============================] - 58s 160ms/step - loss: 0.0127 - accuracy: 0.9515 - val_loss: 0.0288 - val_accuracy: 0.8927\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 41/50\n",
            "362/362 [==============================] - 58s 161ms/step - loss: 0.0128 - accuracy: 0.9487 - val_loss: 0.0285 - val_accuracy: 0.8958\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 42/50\n",
            "362/362 [==============================] - 58s 159ms/step - loss: 0.0125 - accuracy: 0.9470 - val_loss: 0.0277 - val_accuracy: 0.8978\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 43/50\n",
            "362/362 [==============================] - 58s 160ms/step - loss: 0.0119 - accuracy: 0.9559 - val_loss: 0.0283 - val_accuracy: 0.9008\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 44/50\n",
            "362/362 [==============================] - 59s 164ms/step - loss: 0.0120 - accuracy: 0.9573 - val_loss: 0.0289 - val_accuracy: 0.8976\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 45/50\n",
            "362/362 [==============================] - 58s 161ms/step - loss: 0.0111 - accuracy: 0.9585 - val_loss: 0.0278 - val_accuracy: 0.9004\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 46/50\n",
            "362/362 [==============================] - 59s 162ms/step - loss: 0.0119 - accuracy: 0.9548 - val_loss: 0.0292 - val_accuracy: 0.8972\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 47/50\n",
            "362/362 [==============================] - 58s 160ms/step - loss: 0.0110 - accuracy: 0.9602 - val_loss: 0.0303 - val_accuracy: 0.8982\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 48/50\n",
            "362/362 [==============================] - 57s 157ms/step - loss: 0.0110 - accuracy: 0.9600 - val_loss: 0.0303 - val_accuracy: 0.8946\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 49/50\n",
            "362/362 [==============================] - 57s 159ms/step - loss: 0.0108 - accuracy: 0.9607 - val_loss: 0.0301 - val_accuracy: 0.8974\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "Epoch 50/50\n",
            "362/362 [==============================] - 57s 159ms/step - loss: 0.0108 - accuracy: 0.9603 - val_loss: 0.0307 - val_accuracy: 0.8980\n",
            "INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXmta98h5AeB",
        "outputId": "230937a5-6ecb-4403-c4e0-8f5c22663c1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'val'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "c2ny3d36AaDg",
        "outputId": "6b33fd11-291b-4cd7-84eb-2ab50ef8a358"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "test_batch_size = 128\r\n",
        "test_generator = data_generator.generate_images_gender(test_idx, is_training=False, batch_size=test_batch_size)\r\n",
        "gender_pred = model.predict_generator(test_generator, steps=len(test_idx)//test_batch_size)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1976: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Fij3EaUYKK",
        "outputId": "dfcbb394-994b-4306-f7a9-6523a4f5f492"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "test_generator = data_generator.generate_images(test_idx, is_training=False, batch_size=test_batch_size)\r\n",
        "samples = 0\r\n",
        "images, age_true, race_true, gender_true = [], [], [], []\r\n",
        "for test_batch in test_generator:\r\n",
        "    image = test_batch[0]\r\n",
        "    labels = test_batch[1]\r\n",
        "    \r\n",
        "    images.extend(image)\r\n",
        "    age_true.extend(labels[0])\r\n",
        "    race_true.extend(labels[1])\r\n",
        "    gender_true.extend(labels[2])\r\n",
        "    \r\n",
        "age_true = np.array(age_true)\r\n",
        "race_true = np.array(race_true)\r\n",
        "gender_true = np.array(gender_true)\r\n",
        "\r\n",
        "race_true, gender_true = race_true.argmax(axis=-1), gender_true.argmax(axis=-1)\r\n",
        "gender_pred = gender_pred.argmax(axis=-1)\r\n",
        "\r\n",
        "age_true = age_true * data_generator.max_age"
      ],
      "outputs": [],
      "metadata": {
        "id": "gyEiqloIU88y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "cr_gender = classification_report(gender_true, gender_pred, target_names=dataset_dict['gender_alias'].keys())\r\n",
        "print(cr_gender)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        male       0.91      0.90      0.90      3672\n",
            "      female       0.89      0.90      0.89      3368\n",
            "\n",
            "    accuracy                           0.90      7040\n",
            "   macro avg       0.90      0.90      0.90      7040\n",
            "weighted avg       0.90      0.90      0.90      7040\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOdJh_kMVrka",
        "outputId": "ef55b482-5733-4301-dc7f-1ec7f7674e35"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_prediction = pd.DataFrame({'age_true': age_true, 'race_true': race_true, 'gender_true': gender_true, 'gender_pred': gender_pred})\r\n",
        "\r\n",
        "df_prediction.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age_true  race_true  gender_true  gender_pred\n",
              "0      25.0          2            1            1\n",
              "1      18.0          0            1            1\n",
              "2      38.0          1            0            0\n",
              "3      36.0          0            0            0\n",
              "4      54.0          0            0            0"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age_true</th>\n",
              "      <th>race_true</th>\n",
              "      <th>gender_true</th>\n",
              "      <th>gender_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>36.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>54.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "gZ3Aw7PGXzxS",
        "outputId": "60c40f94-c714-4c7a-a77a-e0a943d5c93e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_prediction.to_csv('df_predctions.csv')"
      ],
      "outputs": [],
      "metadata": {
        "id": "niYf_nJcYv-B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "test_im = data_generator.preprocess_image('/content/23_1_0_20170104021538021.jpg.chip.jpg')\r\n",
        "test_im = np.array([test_im])\r\n",
        "\r\n",
        "prediction = model.predict(test_im)\r\n",
        "print(prediction)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.9459781e-05 9.9996686e-01]]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHa45NG5IH2h",
        "outputId": "081de683-4e8e-4c1e-e394-81a415825c67"
      }
    }
  ]
}